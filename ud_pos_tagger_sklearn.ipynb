{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger Trained on the UD Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/soutsios/pos_tagger_mlp/blob/master/pos_tagger_mlp.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Functions\n",
    "\n",
    "These functions are useful to visualize the training dynamics of the learning algorithm and the confusion matrix of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll, nltk, datetime, warnings\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(f1,\n",
    "                          cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          i=1):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_ENGLISH_TRAIN = '../UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "UD_ENGLISH_DEV = '../UD_English-EWT/en_ewt-ud-dev.conllu'\n",
    "UD_ENGLISH_TEST = '../UD_English-EWT/en_ewt-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    original_sentences=[]\n",
    "    t=0\n",
    "    for sentence in data:\n",
    "        original_sentences.append(sentence.text)\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos:\n",
    "                t+=1\n",
    "                tagged_sentence.append((token.form if token.form else '*None*', token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences, original_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train, development and test set in the appropriate tagged format, tuple (word, pos-tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_original = read_conllu(UD_ENGLISH_TRAIN)\n",
    "val_sentences, val_original = read_conllu(UD_ENGLISH_DEV)\n",
    "test_sentences, test_original = read_conllu(UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important observation: how many terms are in validation set and not found in train set? (This estimates the Out-of-vocabulary rate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = build_vocab(train_sentences)\n",
    "list_2 = build_vocab(val_sentences)\n",
    "diff_list = [item for item in list_2 if item not in list_1]\n",
    "print('Number of terms not found in train set:',len(diff_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We use the \"classical\" machine learning approach: we will train a token classifier model. The classifier gets as input a feature vector describing each token in the sentence. We decide a priori which features are informative to make the tagging decision. In this case, we use a combination of \"word shape\" features which approximate morphological knowledge. We naturally also include lexical information (the token form itself), and some form of \"syntactic knowledge\" by adding reference to the previous and next word in each token feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'word_lower': sentence[index].lower(),\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1].lower(),\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1].lower(),\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scikit-learn model, we model a dataset as a pair of two data structures:\n",
    "* The list of feature dictionaries X (one feature dictionary for each token)\n",
    "* The list of predicted label y (one tag for each token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index)])\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence represented as a list of tokens, return the list of feature dictionaries using our feature encoding method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([features_basic(sentence, index)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test untag()\n",
    "\n",
    "We use untag() to extract raw sentences from the annotated CoNLL dataset. This way we can reproduce a sentence without tags, submit it to the tagger and compare predictions to the gold tags that are provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untag(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply a generic machine learning algorithm (such as Logistic Regression), we need to encode the dataset into a vectorized format.\n",
    "\n",
    "We proceed in two steps: feature engineering and vectorization.\n",
    "\n",
    "For each token, we create a dictionary of features that depend on the sentence from which the token is extracted. \n",
    "These features include the word itself, the word before and the word after, letter suffixes and prefixes, etc.\n",
    "\n",
    "In the scikit-learn approach, before we can use a generic machine learning algorithm, we must then \"vectorize\" the feature dictionaries into vector encodings.\n",
    "For example, lexical features are encoded into one-hot vectors whose dimension is the size of the vocabulary.\n",
    "Note the difference between the method `fit_transform` of the vectorizer, which \"learns\" how to vectorize features, and `transform` which applies a learned vectorizer to feature dictionaries.  We use `fit_transform` on the training data, and `transform` on the other sections (validation and test).\n",
    "\n",
    "These vector representations are what is passed to the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, val, test):\n",
    "\n",
    "    print('Feature encoding method')\n",
    "    print('Vectorizing Dataset...')\n",
    "    print('Vectorizing train...')\n",
    "    X_train, y_train = transform_to_dataset(train)\n",
    "    v = DictVectorizer(sparse=True) \n",
    "    X_train = v.fit_transform([x[0] for x in X_train])\n",
    "    \n",
    "    print('Vectorizing validation...')\n",
    "    X_val, y_val = transform_to_dataset(val)\n",
    "    X_val = v.transform([x[0] for x in X_val])        \n",
    "    \n",
    "    print('Vectorizing test...')\n",
    "    X_test, y_test = transform_to_dataset(test)\n",
    "    X_test = v.transform([x[0] for x in X_test])\n",
    "    \n",
    "    print('Dataset vectorized.')\n",
    "    print('Train shape:', X_train.shape)\n",
    "    print('Validation shape:', X_val.shape)\n",
    "    print('Test shape:', X_test.shape)\n",
    "    \n",
    "    # Compress sparse matrices\n",
    "    X_train = X_train \n",
    "    X_val = X_val \n",
    "    X_test = X_test\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a \"real\" machine learning algorithm using scikit-learn, we will repeat the very simple statistical method we discussed in class.\n",
    "We train and evaluate the Baseline Unigram tagger to compare performance with the tagger we will train next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "unigram_tagger = nltk.UnigramTagger(train_sentences+val_sentences, backoff=default_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [item for sublist in tag_sequence(train_sentences+val_sentences) for item in sublist]\n",
    "y_test = [item for sublist in tag_sequence(test_sentences) for item in sublist]\n",
    "classes = sorted(list(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tag_list(apply_tagger(unigram_tagger, test_sentences))\n",
    "print(\"Accuracy: {0:.4f}\".format(unigram_tagger.accuracy(test_sentences)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, zero_division=1, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=1, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what errors the Baseline tagger makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tagger(tagged_sentence):\n",
    "    note = ''\n",
    "    for tup in list(zip(unigram_tagger.tag(untag(tagged_sentence)),untag_pos(tagged_sentence))):\n",
    "        if tup[0][1]!=tup[1]: note='<<--- Error!'\n",
    "        print(tup[0], tup[1], note)\n",
    "        note=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tagger(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes mistakes. Unsurprising given its simplistic approach and the small size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(f1_score(y_test, y_pred, average='macro'), cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move to a more serious machine learning model. We will train a Logistic Regression model using our feature extraction function based on our \"expertise\" in the domain.\n",
    "\n",
    "We first transform the whole dataset from the CoNLL format into the scikit-learn vectorized encoding using our feature extraction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, vec = vectorize(train_sentences, val_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression algorithm uses a hyper-parameter called C.  We tune the value of this parameter by testing different values on a subset of the training data and observing the impact of the C parameter on selected metrics (accuracy and F1).\n",
    "\n",
    "Because we will use cross-validation, we can use the full train set (train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vstack((X_train, X_val))\n",
    "y_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(train, test, y_train, y_test, scores, estimator, parameters, cv):\n",
    "    print(\"# Estimator:\",estimator)\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)        \n",
    "        clf = GridSearchCV(estimator, parameters, cv=cv, scoring='%s' % score)\n",
    "        clf.fit(train, y_train)\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        print()\n",
    "        print(\"Detailed classification report:\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        y_pred = clf.predict(test)\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        print('Accuracy: {0:0.4f}   f1-score: {1:0.4f} \\n'.format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregr = LogisticRegression(solver='liblinear', random_state=13)\n",
    "# Cross validation strategy\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# Scores could also be ['precision', 'recall', ....]\n",
    "scores = ['accuracy', 'f1_macro']\n",
    "\n",
    "params = [{'C': [0.1, 1, 2, 3, 4, 5, 10, 20, 50]}]\n",
    "#logregr = hyper_tuning(X_train, X_test, y_train, y_test, scores, logregr, params, skf)\n",
    "#You may want to comment previous line and comment-out next lines to see how hyper-tuning works and dont wait too much time...\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "logregr = hyper_tuning(X_train[:500], X_test[:50], y_train[:500], y_test[:50], scores, logregr, params, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now training using the best hyper-parameter selected above.  This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ini = datetime.datetime.now()\n",
    "print('Training...')\n",
    "clf = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(clf.score(X_test, y_test)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Types of Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "k=0\n",
    "i2w = id2word(test_sentences)\n",
    "error_counter = collections.Counter()\n",
    "for i in range(X_test.shape[0]):\n",
    "    correct_tag_id = y_test\n",
    "    if y_pred[i]!=y_test[i]:\n",
    "        k += 1\n",
    "        word = i2w[i]\n",
    "        error_counter[word] += 1\n",
    "print('Accuracy: {0:.4f}'.format((len(i2w)-k)/len(i2w)))\n",
    "print('Total errors/Total words: {}/{}\\n'.format(k,len(i2w)))\n",
    "print('Most common errors:',error_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates the \"training dynamics\" of the LR model: how fast does it improve as it keeps training. Originally, the difference between the test dataset and the cross-validation (on part of the test data) is large; as training proceeds, the gap reduces. This diagram is important to verify we do not have a case of over-fitting - where the model does \"very well\" on training data and does not improve on test data.  \n",
    "\n",
    "This computation takes a long time (as we keep training and evaluating multiple times to obtain the snapshots). It is not necessary to run the rest of the notebook so that you can safely skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_learning_curve(clf, 'Logistic Regression', X_train, y_train, ylim=(0.7, 1.01), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Logistic Regression'\n",
    "plot_confusion_matrix(f1, cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag an Unknown Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try our trained tagger on new sentences.  To tag a sentence given as a string, we must apply the following steps:\n",
    "* Tokenize the string into a list of tokens\n",
    "* Turn each token into a features dictionary (using the features used by our model)\n",
    "* Turn the list of feature dictionaries into vectors (using scikit-learn vectorization method)\n",
    "* Pass the resulting matrix (one row vector for each token) to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download the nltk model for sentence tokenizer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize('Word embeddings provide a dense representation of words and their relative meanings.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = transform_test_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vec.transform([x[0] for x in X_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorized sentence is a sparse matrix with one row for each token and columns for the vectorized features\n",
    "# For example, if the vocabulary has 1000 unique words, the vectorized sentence will have 1000 columns for each word feature.\n",
    "# This is a very sparse matrix, where most of the values are zero.\n",
    "X_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.str_ is a subclass of str that is used to represent string arrays in NumPy.\n",
    "print('Here is what our LR tagger predicts for the test sentence:\\n',list(zip(tokens, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this process into a prediction function from a sentence encoded as a single string to a list of pairs (token, predicted_tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "   tokens = nltk.word_tokenize(sentence)\n",
    "   X_features = transform_test_sentence(tokens)\n",
    "   X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "   pred = clf.predict(X_vectorized)\n",
    "   return list(zip(tokens, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence(\"Let me join the chorus of annoyance over Google's new toolbar , which, as noted in the linked article, commits just about every sin an online marketer could commit, and makes up a few new ones besides.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Hard Sentences\n",
    "\n",
    "Hard sentences are sentences that contain multiple wrongly predicted tags given our classifier.\n",
    "\n",
    "Write code to collect hard sentences given a classifier clf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_in_sentence_prediction(clf, tagged_sentence):\n",
    "    \"\"\"Given a tagged sentence from the dataset, return the number of errors and the predicted tags.\"\"\"\n",
    "    errors = 0\n",
    "    tokens = [word for word, _ in tagged_sentence] \n",
    "    true_tags = [true_tag for _, true_tag in tagged_sentence] \n",
    "\n",
    "    X_features = transform_test_sentence(tokens)\n",
    "    X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "    pred = clf.predict(X_vectorized)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if true_tags[i] != pred[i]:\n",
    "            errors += 1\n",
    "\n",
    "    return errors, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_sentences = []\n",
    "idx = 0\n",
    "for s in test_sentences:\n",
    "    errors, pred = errors_in_sentence_prediction(clf, s)\n",
    "    if errors > 0:\n",
    "        hard_sentences.append((s, errors, pred, idx))\n",
    "    idx += 1\n",
    "print(f'Number of sentences with errors: {len(hard_sentences)} out of {len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a histogram showing how the sentences in the test dataset are distributed in terms of prediction errors per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a histogram of the number of errors per sentence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([x[1] for x in hard_sentences], bins=16, edgecolor='black')\n",
    "plt.title('Number of errors per sentence')\n",
    "plt.xlabel('Number of errors')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction errors for sentences with more than 5 errors.\n",
    "for s in hard_sentences:\n",
    "    if s[1] > 5:\n",
    "        print(\" \".join(untag(s[0])))\n",
    "        print('Number of errors:', s[1])\n",
    "        for i in range(len(s[0])):\n",
    "            if s[0][i][1] != s[2][i]:\n",
    "                print(f'{s[0][i][0]:<20}  C: {s[0][i][1]:<12}  P: {s[2][i]:<12} **** Error')\n",
    "            else:\n",
    "                print(f'{s[0][i][0]:<23}  {s[0][i][1]:<12}')\n",
    "        print(40*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "1. Identify tokens that are misclassified more than 10 times in the test set. Print the sentences where the errors are predicted (about 100 sentences).\n",
    "2. Provide a possible reason why these errors are made by the tagger based on your understanding of the knowledge needed to correctly tag these tokens.\n",
    "3. Based on this error analysis, invent five sentences that are badly tagged. Explain what is your method to create these hard examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = collections.defaultdict(list)\n",
    "sen_ids = [] \n",
    "for i, s in enumerate(hard_sentences):\n",
    "    words = [word for word, _ in hard_sentences[i][0]]\n",
    "    full_s = ' '.join(words)\n",
    "    sen_ids.append(full_s)\n",
    "    for j in range(len(s[0])):\n",
    "        if s[0][j][1] != s[2][j]:\n",
    "            tokens[s[0][j][0]].append(i)\n",
    "\n",
    "sentences = set()\n",
    "print(\"Tokens with more than 10 errors:\")\n",
    "for token, lst in tokens.items():\n",
    "    if len(lst) > 10:\n",
    "        print(token)\n",
    "        for index in lst:\n",
    "            sentences.add(sen_ids[index])\n",
    "\n",
    "print(\"Total amount of sentences:\", len(sentences))\n",
    "\n",
    "print(\"Sentences where the errors are predicted:\\n\")\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These errors are most likely made by the tagger due to their high likeliness of morphological amiguity and the high frequencies that they appear.\n",
    "The words that appear in this list have multiple possible pos tags / gramatical functions.\n",
    "\n",
    "For example:\n",
    "\n",
    "that: Can be a demonstrative determiner (DET), a pronoun (PRON), or a subordinating conjunction (SCONJ)\n",
    "\n",
    "as: Switches between subordinating conjunction (SCONJ), adposition (ADP), and adverb (ADV)\n",
    "\n",
    "to: Often confused between particle (PART) and adposition (ADP)\n",
    "\n",
    "like: Has many possible tags (SCONJ, ADP, VERB, ADJ)\n",
    "etc.\n",
    "\n",
    "\"01-Feb-02\" is probably OOV and our model does not handle these cases well due to not using a subword tokenizer and its limited context window.\n",
    "\n",
    "The logistic regression model lacks the ability to capture long-range dependencies and hierarchical sentence structure needed to consistently disambiguate these challenging cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "i. \"That book that I read stated that that theory was flawed.\"\n",
    "\n",
    "Method: Using \"that\" in multiple roles (DET, PRON, SCONJ, DET) in a single sentence to confuse the tagger.\n",
    "\n",
    "\n",
    "ii. \"As I waited for the train, I stood as still as a statue for what felt like hours.\"\n",
    "\n",
    "Method: Using \"as\" and \"for\" in different syntactic roles that require broader context to disambiguate correctly.\n",
    "\n",
    "\n",
    "iii. \"The stock went up as prices went down, but investors need to figure out how to build up reserves.\"\n",
    "\n",
    "Method: Including directional words (\"up\", \"down\", \"out\", \"up\") in different contexts (ADV, PART, ADP) to exploit the tagger's confusion with these terms.\n",
    "\n",
    "\n",
    "iv. \"I'd like a jacket like that one, since companies like Nike make products with a sport-like appearance.\"\n",
    "\n",
    "Method: Using \"like\" in three different roles (VERB, ADP, ADJ) to challenge the tagger's ability to distinguish these uses.\n",
    "\n",
    "v. \"We went skiing on 17-Jan-23 in Salt Lake City.\"\n",
    "\n",
    "Method: Using \"17-Jan-23\" which is most likely oov in our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "import json  \n",
    "\n",
    "\n",
    "model = 'grok-3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Universal Dependencies POS Tagset (17 core tags) as an enum ---\n",
    "class UDPosTag(str, Enum):\n",
    "    ADJ = \"ADJ\"     # adjective\n",
    "    ADP = \"ADP\"     # adposition\n",
    "    ADV = \"ADV\"     # adverb\n",
    "    AUX = \"AUX\"     # auxiliary verb\n",
    "    CCONJ = \"CCONJ\" # coordinating conjunction\n",
    "    DET = \"DET\"     # determiner\n",
    "    INTJ = \"INTJ\"   # interjection\n",
    "    NOUN = \"NOUN\"   # noun\n",
    "    NUM = \"NUM\"     # numeral\n",
    "    PART = \"PART\"   # particle\n",
    "    PRON = \"PRON\"   # pronoun\n",
    "    PROPN = \"PROPN\" # proper noun\n",
    "    PUNCT = \"PUNCT\" # punctuation\n",
    "    SCONJ = \"SCONJ\" # subordinating conjunction\n",
    "    SYM = \"SYM\"     # symbol\n",
    "    VERB = \"VERB\"   # verb\n",
    "    X = \"X\"         # other / unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for structured output\n",
    "class TokenPOS(BaseModel):\n",
    "    text: str = Field(description=\"The token text\")\n",
    "    pos_tag: UDPosTag = Field(description=\"The Universal Dependencies POS tag\")\n",
    "\n",
    "class SentencePOS(BaseModel):\n",
    "    tokens: List[TokenPOS] = Field(description=\"List of tokens with their POS tags\")\n",
    "\n",
    "class TaggedSentences(BaseModel):\n",
    "    \"\"\"Represents a list of sentences with their tagged tokens.\"\"\"\n",
    "    sentences: List[SentencePOS] = Field(description=\"A list of sentences, each containing tagged tokens.\")\n",
    "\n",
    "# --- Configure the Grok API ---\n",
    "\n",
    "try:\n",
    "    # Attempt to get API key from environment variable\n",
    "    def load_env_from_ini(filename):\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "    # Load the API key\n",
    "    load_env_from_ini(\"grok_key.ini\")\n",
    "    api_key = os.environ.get(\"GROK_API_KEY\")\n",
    "    if not api_key:\n",
    "        # Fallback or specific instruction for local setup\n",
    "        api_key = \"YOUR_API_KEY\"\n",
    "        if api_key == \"YOUR_API_KEY\":\n",
    "           print(\"⚠️ Warning: API key not found in environment variables. Using placeholder.\")\n",
    "           print(\"   Please set the GROK_API_KEY environment variable or replace 'YOUR_API_KEY' in the code.\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://api.x.ai/v1\",\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring API: {e}\")\n",
    "    print(\"Please ensure you have a valid API key set.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first attempt to prompt the model, we sent the list of sentences that we would like to tag in a JSON list format. ie - [sent1, sent2, ...]\n",
    "\n",
    "We noticed that the sentences were not being segmented the same way as the lr tagger was recieving the sentences to tag so we decided to send the model already tokenized sentences. We sent a nested JSON list of tokeized sentences. \n",
    "ie - [[tok1, tok2, ...], [tok1, tok2, ...], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentences_ud(sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on the input list of sentences using the Grok API and\n",
    "    returns the result structured according to the TaggedSentences Pydantic model.\n",
    "    \n",
    "    Args:\n",
    "        sentences_json: JSON string containing one or more sentences to tag\n",
    "        \n",
    "    Returns:\n",
    "        A TaggedSentences object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Construct the prompt with JSON input format\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions that outputs precise structured JSON.\n",
    "    \n",
    "    I will provide you with a JSON array of sentences to tag. Process each sentence separately.\n",
    "    \n",
    "    Tag each token with Universal Dependencies (UD) POS tags:\n",
    "    ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner, \n",
    "    INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun, \n",
    "    PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other\n",
    "    \n",
    "    Rules:\n",
    "    - Split on whitespace and punctuation (except in URLs, numbers, abbreviations)\n",
    "    - Split contractions: \"don't\" → [\"Do\", \"n't\"], \"it's\" → [\"It\", \"'s\"]\n",
    "    - Separate possessives: \"Elena's\" → [\"Elena\", \"'s\"]\n",
    "    - Split hyphenated compounds: \"search-engine\" → [\"search\", \"-\", \"engine\"]\n",
    "    - Keep punctuation as separate tokens\n",
    "    - Preserve numbers with internal periods/commas (e.g., 3.14, 1,000)\n",
    "    - Do not merge words except for contractions/clitics\n",
    "    - if there is a sentence equal to \"...\" tag it as \"PUNCT\", don't ignore it.\n",
    "    - make sure you don't skip any tokens or strings. tag all tokens.\n",
    "    \n",
    "    Input JSON array of sentences:\n",
    "    {sentences_json}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag(sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    return tag_sentences_ud(sentences_json)\n",
    "\n",
    "def batch_tag_sentences_ud(sentences: List[str], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"Process sentences in parallel with rate limiting using JSON formatting\"\"\"\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        # Convert batch to JSON string \n",
    "        batch_json = json.dumps(batch)\n",
    "        batches.append(batch_json)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches with parallel workers...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag, batch_json): i \n",
    "                          for i, batch_json in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "    \n",
    "\n",
    "llm_hard_sentences = []\n",
    "llm_error_data = []\n",
    "\n",
    "def evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5):\n",
    "    \"\"\"\n",
    "    Evaluates the LLM tagger results against ground truth, handling potential misalignments.\n",
    "    \n",
    "    Args:\n",
    "        results_llm: List of TaggedSentences results from the LLM\n",
    "        lr_hard_sentences: List of (sentence, errors, lr_tags, idx) tuples with ground truth\n",
    "        batch_size: The batch size used when processing sentences\n",
    "    \n",
    "    Returns:\n",
    "        Metrics for the LLM tagger performance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out None results\n",
    "    results_llm = [r for r in results_llm if r is not None]\n",
    "    \n",
    "    # Create batch-aware mapping of results\n",
    "    batched_sentences = [lr_hard_sentences[i:i+batch_size] for i in range(0, len(lr_hard_sentences), batch_size)]\n",
    "    \n",
    "    # Initialize empty lists for predictions and words\n",
    "    llm_pred = []\n",
    "    llm_words = []\n",
    "    sentence_mapping = {}  # Maps original index to result index\n",
    "    \n",
    "    print(f\"Type of results_llm[0]: {type(results_llm[0]) if results_llm else 'No results'}\")\n",
    "    \n",
    "    # Process each batch separately\n",
    "    for batch_idx, batch in enumerate(batched_sentences):\n",
    "        if batch_idx >= len(results_llm):\n",
    "            print(f\"Missing results for batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        batch_result = results_llm[batch_idx]\n",
    "        \n",
    "        # Check if it has sentences attribute\n",
    "        if hasattr(batch_result, 'sentences'):\n",
    "            result_sentences = batch_result.sentences\n",
    "        elif hasattr(batch_result, 'tokens'):\n",
    "            # Handle single-sentence result case\n",
    "            result_sentences = [batch_result]\n",
    "        else:\n",
    "            print(f\"Unknown structure for batch {batch_idx}: {dir(batch_result)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if we got the expected number of sentences\n",
    "        if len(result_sentences) != len(batch):\n",
    "            print(f\"Warning: Batch {batch_idx} returned {len(result_sentences)} sentences, expected {len(batch)}\")\n",
    "        \n",
    "        # Map each input sentence to its result (or None if missing)\n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(batch):\n",
    "            orig_idx = batch_idx * batch_size + i  # Original index in lr_hard_sentences\n",
    "            \n",
    "            if i < len(result_sentences):\n",
    "                # We have a result for this sentence\n",
    "                pred = [token.pos_tag for token in result_sentences[i].tokens]\n",
    "                words = [token.text for token in result_sentences[i].tokens]\n",
    "                llm_pred.append(pred)\n",
    "                llm_words.append(words)\n",
    "                sentence_mapping[orig_idx] = len(llm_pred) - 1\n",
    "            else:\n",
    "                # No result for this sentence\n",
    "                print(f\"No prediction available for sentence {orig_idx}\")\n",
    "                # Don't add to llm_pred/llm_words, but record this in mapping\n",
    "                sentence_mapping[orig_idx] = None\n",
    "    \n",
    "    # Only proceed with metrics if we have predictions\n",
    "    if len(llm_pred) > 0:\n",
    "        # Initialize counters and lists\n",
    "        global llm_hard_sentences\n",
    "        global llm_error_data\n",
    "        llm_hard_sentences = []\n",
    "        fixed_by_llm = 0\n",
    "        new_errors_by_llm = 0\n",
    "        llm_error_data = []\n",
    "        mismatches = 0\n",
    "        \n",
    "        # For metrics calculation\n",
    "        all_true_tags = []\n",
    "        all_pred_tags = []\n",
    "        all_words = []\n",
    "        \n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(lr_hard_sentences):\n",
    "            # Use the mapping to get the correct prediction index\n",
    "            if i not in sentence_mapping or sentence_mapping[i] is None:\n",
    "                print(f\"Skipping sentence {i} - no prediction available\")\n",
    "                continue\n",
    "                \n",
    "            pred_idx = sentence_mapping[i]\n",
    "            words = [word for word, _ in s]\n",
    "            true_tags = [true_tag for _, true_tag in s]\n",
    "            pred_tags = llm_pred[pred_idx]\n",
    "            \n",
    "            # Sanity check: length of tokens should match\n",
    "            if len(true_tags) != len(pred_tags) or len(pred_tags) != len(words):\n",
    "                print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, pred={len(pred_tags)}\")\n",
    "                # print(llm_words[pred_idx])\n",
    "                # print(words)\n",
    "                mismatches += 1\n",
    "                continue\n",
    "            \n",
    "            # Add to the collections for metrics calculations\n",
    "            all_true_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in true_tags])\n",
    "            all_pred_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in pred_tags])\n",
    "            all_words.extend(words)\n",
    "\n",
    "\n",
    "            llm_errors = 0\n",
    "            for j in range(len(words)):\n",
    "                if true_tags[j] != pred_tags[j]:\n",
    "                    llm_errors += 1\n",
    "                    llm_error_data.append((words[j], true_tags[j], pred_tags[j]))\n",
    "                    \n",
    "                lr_wrong = lr_tags[j] != true_tags[j]\n",
    "                llm_wrong = pred_tags[j] != true_tags[j]\n",
    "                \n",
    "                if lr_wrong and not llm_wrong:\n",
    "                    fixed_by_llm += 1\n",
    "                if not lr_wrong and llm_wrong:\n",
    "                    new_errors_by_llm += 1\n",
    "                    \n",
    "            if llm_errors > 0:\n",
    "                llm_hard_sentences.append((s, llm_errors))\n",
    "        \n",
    "        # Print comparison metrics\n",
    "        print(f'Number of sentences with errors (llm): {len(llm_hard_sentences)} out of {len(lr_hard_sentences)}')\n",
    "        print(f'Number of sentences with errors (lr): {len(lr_hard_sentences)}')\n",
    "        print(f\"✅ Errors fixed by LLM: {fixed_by_llm}\")\n",
    "        print(f\"⚠️ New errors made by LLM: {new_errors_by_llm}\")\n",
    "        print(f\"❗️ Mismatches in sentence length: {mismatches}\")\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        if len(all_true_tags) > 0:\n",
    "            print(\"\\n--- LLM Tagger Token-Level Metrics ---\")\n",
    "            llm_accuracy = accuracy_score(all_true_tags, all_pred_tags)\n",
    "            llm_f1_macro = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            print(f\"Accuracy: {llm_accuracy:.4f}\")\n",
    "            print(f\"F1-macro score: {llm_f1_macro:.4f}\")\n",
    "            \n",
    "            # Generate classification report\n",
    "            print(\"\\nClassification Report for LLM Tagger:\")\n",
    "            print(classification_report(all_true_tags, all_pred_tags, digits=4))\n",
    "            \n",
    "            # Generate confusion matrix\n",
    "            classes = sorted(list(set(all_true_tags)))\n",
    "            cnf_matrix = confusion_matrix(all_true_tags, all_pred_tags)\n",
    "            f1 = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            plot_confusion_matrix(f1, cnf_matrix, target_names=classes, \n",
    "                                title='Confusion matrix for LLM Tagger', normalize=False)\n",
    "            \n",
    "            # Count and display most common errors\n",
    "            error_counter = collections.Counter()\n",
    "            total_errors = 0\n",
    "            for i in range(len(all_true_tags)):\n",
    "                if all_true_tags[i] != all_pred_tags[i]:\n",
    "                    total_errors += 1\n",
    "                    word = all_words[i]\n",
    "                    error_counter[word] += 1\n",
    "            \n",
    "            print(\"\\nFrequent Types of Mistakes:\")\n",
    "            print(f'Accuracy: {(len(all_true_tags)-total_errors)/len(all_true_tags):.4f}')\n",
    "            print(f'Total errors/Total words: {total_errors}/{len(all_true_tags)}\\n')\n",
    "            print('Most common errors:', error_counter.most_common(20))\n",
    "            \n",
    "            return {\n",
    "                'accuracy': llm_accuracy,\n",
    "                'f1_macro': llm_f1_macro,\n",
    "                'fixed_by_llm': fixed_by_llm,\n",
    "                'new_errors_by_llm': new_errors_by_llm,\n",
    "                'mismatches': mismatches,\n",
    "                'error_data': llm_error_data,\n",
    "                'hard_sentences': llm_hard_sentences\n",
    "            }\n",
    "        else:\n",
    "            print(\"No data available to calculate metrics\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No LLM predictions available to calculate metrics\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_hard_sentences = [s for s in hard_sentences if 1 <= s[1] <= 3]\n",
    "# sentences = [\" \".join(untag(pairs)) for pairs, _, _ , _ in lr_hard_sentences]\n",
    "# results_llm = batch_tag_sentences_ud(sentences, batch_size=5)\n",
    "# metric = evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "    \n",
    "\n",
    "def evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5, collect_error_context=True):\n",
    "    \"\"\"\n",
    "    Evaluates the LLM tagger results against ground truth, and collects comprehensive error data for analysis.\n",
    "    \n",
    "    Args:\n",
    "        results_llm: List of TaggedSentences results from the LLM\n",
    "        lr_hard_sentences: List of (sentence, errors, lr_tags, idx) tuples with ground truth\n",
    "        batch_size: The batch size used when processing sentences\n",
    "        collect_error_context: Whether to collect detailed context for error analysis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics and error analysis data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out None results\n",
    "    results_llm = [r for r in results_llm if r is not None]\n",
    "    \n",
    "    # Create batch-aware mapping of results\n",
    "    batched_sentences = [lr_hard_sentences[i:i+batch_size] for i in range(0, len(lr_hard_sentences), batch_size)]\n",
    "    \n",
    "    llm_pred = []\n",
    "    llm_words = []\n",
    "    sentence_mapping = {}  # Maps original index to result index\n",
    "    \n",
    "    # Process all batches\n",
    "    for batch_idx, batch in enumerate(batched_sentences):\n",
    "        if batch_idx >= len(results_llm):\n",
    "            print(f\"Missing results for batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        batch_result = results_llm[batch_idx]\n",
    "        \n",
    "        # Check if it has sentences attribute\n",
    "        if hasattr(batch_result, 'sentences'):\n",
    "            result_sentences = batch_result.sentences\n",
    "        elif hasattr(batch_result, 'tokens'):\n",
    "            # Handle single-sentence result case\n",
    "            result_sentences = [batch_result]\n",
    "        else:\n",
    "            print(f\"Unknown structure for batch {batch_idx}: {dir(batch_result)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if we got the expected number of sentences\n",
    "        if len(result_sentences) != len(batch):\n",
    "            print(f\"Warning: Batch {batch_idx} returned {len(result_sentences)} sentences, expected {len(batch)}\")\n",
    "        \n",
    "        # Map each input sentence to its result (or None if missing)\n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(batch):\n",
    "            orig_idx = batch_idx * batch_size + i  # Original index in lr_hard_sentences\n",
    "            \n",
    "            if i < len(result_sentences):\n",
    "                # We have a result for this sentence\n",
    "                pred = [token.pos_tag for token in result_sentences[i].tokens]\n",
    "                words = [token.text for token in result_sentences[i].tokens]\n",
    "                llm_pred.append(pred)\n",
    "                llm_words.append(words)\n",
    "                sentence_mapping[orig_idx] = len(llm_pred) - 1\n",
    "            else:\n",
    "                # No result for this sentence\n",
    "                print(f\"No prediction available for sentence {orig_idx}\")\n",
    "                # Don't add to llm_pred/llm_words, but record this in mapping\n",
    "                sentence_mapping[orig_idx] = None\n",
    "    \n",
    "    # Only create metrics if we have predictions\n",
    "    if len(llm_pred) > 0:\n",
    "        global llm_hard_sentences\n",
    "        global llm_error_data\n",
    "        llm_hard_sentences = []\n",
    "        fixed_by_llm = 0\n",
    "        new_errors_by_llm = 0\n",
    "        llm_error_data = []\n",
    "        detailed_error_data = []  # For error analysis with context\n",
    "        mismatches = 0\n",
    "        \n",
    "        # For metrics calculation\n",
    "        all_true_tags = []\n",
    "        all_pred_tags = []\n",
    "        all_words = []\n",
    "        \n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(lr_hard_sentences):\n",
    "            # Use the mapping to get the correct prediction index\n",
    "            if i not in sentence_mapping or sentence_mapping[i] is None:\n",
    "                print(f\"Skipping sentence {i} - no prediction available\")\n",
    "                continue\n",
    "                \n",
    "            pred_idx = sentence_mapping[i]\n",
    "            words = [word for word, _ in s]\n",
    "            true_tags = [true_tag for _, true_tag in s]\n",
    "            pred_tags = llm_pred[pred_idx]\n",
    "            \n",
    "            # Sanity check- length of tokens should match\n",
    "            if len(true_tags) != len(pred_tags) or len(pred_tags) != len(words):\n",
    "                print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, pred={len(pred_tags)}\")\n",
    "                mismatches += 1\n",
    "                continue\n",
    "            \n",
    "            # for metrics calculations\n",
    "            all_true_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in true_tags])\n",
    "            all_pred_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in pred_tags])\n",
    "            all_words.extend(words)\n",
    "\n",
    "            llm_errors = 0\n",
    "            sentence_text = \" \".join(words)\n",
    "            \n",
    "            for j in range(len(words)):\n",
    "                true_tag = true_tags[j]\n",
    "                pred_tag = pred_tags[j]\n",
    "                word = words[j]\n",
    "                \n",
    "                if true_tag != pred_tag:\n",
    "                    llm_errors += 1\n",
    "                    llm_error_data.append((word, true_tag, pred_tag))\n",
    "                    \n",
    "                    # Enhanced error data with context for error analysis\n",
    "                    if collect_error_context:\n",
    "                        detailed_error_data.append({\n",
    "                            \"word\": word,\n",
    "                            \"correct_tag\": true_tag.value if hasattr(true_tag, 'value') else str(true_tag),\n",
    "                            \"predicted_tag\": pred_tag.value if hasattr(pred_tag, 'value') else str(pred_tag),\n",
    "                            \"sentence_context\": sentence_text,\n",
    "                            \"word_index\": j,\n",
    "                            # Store surrounding words for context window\n",
    "                            \"context_window\": words[max(0, j-3):min(len(words), j+4)],\n",
    "                            # Mark if this was also wrong in LR model\n",
    "                            \"also_wrong_in_lr\": lr_tags[j] != true_tag\n",
    "                        })\n",
    "                \n",
    "                lr_wrong = lr_tags[j] != true_tag\n",
    "                llm_wrong = pred_tag != true_tag\n",
    "                \n",
    "                if lr_wrong and not llm_wrong:\n",
    "                    fixed_by_llm += 1\n",
    "                if not lr_wrong and llm_wrong:\n",
    "                    new_errors_by_llm += 1\n",
    "                    \n",
    "            if llm_errors > 0:\n",
    "                llm_hard_sentences.append((s, llm_errors))\n",
    "        \n",
    "        # comparison metrics\n",
    "        print(f'Number of sentences with errors (llm): {len(llm_hard_sentences)} out of {len(lr_hard_sentences)}')\n",
    "        print(f'Number of sentences with errors (lr): {len(lr_hard_sentences)}')\n",
    "        print(f\"✅ Errors fixed by LLM: {fixed_by_llm}\")\n",
    "        print(f\"⚠️ New errors made by LLM: {new_errors_by_llm}\")\n",
    "        print(f\"❗️ Mismatches in sentence length: {mismatches}\")\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        if len(all_true_tags) > 0:\n",
    "            print(\"\\n--- LLM Tagger Token-Level Metrics ---\")\n",
    "            llm_accuracy = accuracy_score(all_true_tags, all_pred_tags)\n",
    "            llm_f1_macro = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            print(f\"Accuracy: {llm_accuracy:.4f}\")\n",
    "            print(f\"F1-macro score: {llm_f1_macro:.4f}\")\n",
    "            \n",
    "            print(\"\\nClassification Report for LLM Tagger:\")\n",
    "            print(classification_report(all_true_tags, all_pred_tags, digits=4))\n",
    "            \n",
    "            classes = sorted(list(set(all_true_tags)))\n",
    "            cnf_matrix = confusion_matrix(all_true_tags, all_pred_tags)\n",
    "            f1 = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            plot_confusion_matrix(f1, cnf_matrix, target_names=classes,\n",
    "                                title='Confusion matrix for LLM Tagger', normalize=False)\n",
    "            \n",
    "            # most common errors\n",
    "            error_counter = collections.Counter()\n",
    "            total_errors = 0\n",
    "            for i in range(len(all_true_tags)):\n",
    "                if all_true_tags[i] != all_pred_tags[i]:\n",
    "                    total_errors += 1\n",
    "                    word = all_words[i]\n",
    "                    error_counter[word] += 1\n",
    "            \n",
    "            print(\"\\nFrequent Types of Mistakes:\")\n",
    "            print(f'Accuracy: {(len(all_true_tags)-total_errors)/len(all_true_tags):.4f}')\n",
    "            print(f'Total errors/Total words: {total_errors}/{len(all_true_tags)}\\n')\n",
    "            print('Most common errors:', error_counter.most_common(20))\n",
    "            \n",
    "            # Return both metrics and enhanced error data for analysis\n",
    "            return {\n",
    "                'accuracy': llm_accuracy,\n",
    "                'f1_macro': llm_f1_macro,\n",
    "                'fixed_by_llm': fixed_by_llm,\n",
    "                'new_errors_by_llm': new_errors_by_llm,\n",
    "                'mismatches': mismatches,\n",
    "                'error_data': llm_error_data,\n",
    "                'detailed_error_data': detailed_error_data, \n",
    "                'hard_sentences': llm_hard_sentences,\n",
    "                'error_counts': error_counter\n",
    "            }\n",
    "        else:\n",
    "            print(\"No data available to calculate metrics\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No LLM predictions available to calculate metrics\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 - 2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from ratelimit import limits, sleep_and_retry \n",
    "\n",
    "def tag_pretokenized_sentences(tokenized_sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on pre-tokenized sentences using the Grok API.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences_json: JSON string containing a list of tokenized sentences, \n",
    "                                  where each sentence is a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        A TaggedSentences object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Construct the prompt with pre-tokenized input format\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions.\n",
    "    \n",
    "    I will provide you with a JSON array where each element is a pre-tokenized sentence (a list of tokens).\n",
    "    Your task is ONLY to assign the correct Universal Dependencies POS tag to each token.\n",
    "    DO NOT modify the tokenization in any way - use exactly the tokens provided.\n",
    "    \n",
    "    Tag each token with one of these Universal Dependencies POS tags:\n",
    "    ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner,\n",
    "    INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun,\n",
    "    PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other\n",
    "    \n",
    "    Important guidelines:\n",
    "    - DO NOT change, merge, or split any tokens - use exactly the tokens provided\n",
    "    - If a token is \"...\" tag it as PUNCT\n",
    "    - Tag all tokens - do not skip any\n",
    "    - Contractions like \"n't\" should be tagged as PART\n",
    "    - Possessive markers like \"'s\" should be tagged as PART\n",
    "    - Proper nouns (names of specific entities) should be tagged as PROPN\n",
    "    - Auxiliary verbs (be, have, do, will, etc.) should be tagged as AUX\n",
    "    \n",
    "    Input JSON array of pre-tokenized sentences:\n",
    "    {tokenized_sentences_json}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag_pretokenized(tokenized_sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    return tag_pretokenized_sentences(tokenized_sentences_json)\n",
    "\n",
    "def batch_tag_pretokenized_sentences(tokenized_sentences: List[List[str]], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Process pre-tokenized sentences in batches using JSON formatting\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of tokenized sentences, where each sentence is a list of tokens\n",
    "        batch_size: Number of sentences to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        List of TaggedSentences results\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for i in range(0, len(tokenized_sentences), batch_size):\n",
    "        batch = tokenized_sentences[i:i+batch_size]\n",
    "        # Convert batch to JSON string\n",
    "        batch_json = json.dumps(batch)\n",
    "        batches.append(batch_json)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches of pre-tokenized sentences...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag_pretokenized, batch_json): i \n",
    "                          for i, batch_json in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "lr_hard_sentences = [s for s in hard_sentences if 1 <= s[1] <= 3]\n",
    "tokenized_sentences = [untag(pairs) for pairs, _, _, _ in lr_hard_sentences]\n",
    "results_llm = batch_tag_pretokenized_sentences(tokenized_sentences, batch_size=5)\n",
    "\n",
    "metrics = evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "\n",
    "# Plot the histogram for LR\n",
    "ax[0].hist([x[1] for x in lr_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[0].set_title(\"LR Tagger Error Histogram\")\n",
    "ax[0].set_xlabel(\"Number of Errors\")\n",
    "ax[0].set_ylabel(\"Number of Sentences\")\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Plot the histogram for LLM\n",
    "ax[1].hist([errors for s, errors in llm_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[1].set_title(\"LLM Tagger Error Histogram\")\n",
    "ax[1].set_xlabel(\"Number of Errors\")\n",
    "ax[1].set_ylabel(\"Number of Sentences\")\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5\n",
    "### Our hypotheses about the difficult cases for the LLM tagger:\n",
    "#### 1. Function Word Ambiguity\n",
    "The most common errors involve function words that can serve multiple grammatical roles ('have', 'that', 'for', 'to'). The LLM seems to struggle with contextual disambiguation of these highly frequent words.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"That that I saw yesterday wasn't that interesting, but that that you mentioned seems fascinating.\"\n",
    "\n",
    "\"For him to succeed, he studied for hours, for the exam was difficult for anyone without preparation.\"\n",
    "\n",
    "#### 2. Auxiliary vs. Main Verb Distinction\n",
    "Words like 'have', 'had', and 'has' appear frequently in the error list. The confusion matrix shows 90 cases where VERB was incorrectly tagged as AUX, suggesting difficulty in distinguishing when these verbs function as auxiliaries versus main verbs.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"I have three cars but have never had to have them repaired since I have maintained them well.\"\n",
    "\n",
    "\"She has a dog that has been trained by someone who has expertise in animal behavior.\"\n",
    "\n",
    "#### 3. Possessive Constructions\n",
    "Possessives like \"'s\", \"my\", \"your\", and \"our\" are common error points, suggesting the LLM struggles with possessive constructions and their correct classification as PART vs. other categories.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"My sister's friend's car's engine is making that noise that's concerning everyone.\"\n",
    "\n",
    "\"Your book's cover is torn, but your dedication to your studies is admirable.\"\n",
    "\n",
    "#### 4. Preposition vs. Conjunction Ambiguity\n",
    "The confusion matrix reveals significant confusion between SCONJ and ADP (71 cases), indicating the model struggles with words that can function as either prepositions or subordinating conjunctions.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"After dinner, we'll discuss what happened after you left.\"\n",
    "\n",
    "\"Since morning I've been working, since I need to finish this project today.\"\n",
    "\n",
    "#### 5. Proper Noun Recognition\n",
    "There's substantial bidirectional confusion between NOUN and PROPN categories (99 NOUN→PROPN errors, 54 PROPN→NOUN errors), indicating challenges with proper noun identification.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"Apple hired a new employee to polish Polish furniture made by French workers in the french quarter.\"\n",
    "\n",
    "\"May I ask if you're available in May to visit Little Rock with little enthusiasm?\"\n",
    "\n",
    "#### 6. Special Characters and Symbols\n",
    "The tagger has a low F1-score for SYM (0.6144), with 20 cases of SYM being misclassified as PUNCT, suggesting difficulty with non-alphanumeric characters.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"The equation x^2 + y^2 = z^2 requires careful analysis at a cost of $50 per hour.\"\n",
    "\n",
    "\"We saw a 25% increase in Q1, followed by a -10% decline, maintaining a 3:1 ratio overall.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "# --- Define the Pydantic model for structured output ---\n",
    "class ErrorExplanation(BaseModel):\n",
    "    word: str = Field(description=\"The word that was incorrectly tagged\")\n",
    "    correct_tag: str = Field(description=\"The correct UD POS tag\")\n",
    "    predicted_tag: str = Field(description=\"The incorrectly predicted tag\")\n",
    "    explanation: str = Field(description=\"Explanation of why the error likely occurred\")\n",
    "    category: str = Field(description=\"Category of error (e.g., 'Ambiguity', 'Capitalization', etc.)\")\n",
    "\n",
    "class BatchErrorExplanation(BaseModel):\n",
    "    explanations: List[ErrorExplanation] = Field(description=\"List of error explanations\")\n",
    "\n",
    "def explain_tagging_errors(\n",
    "    client,\n",
    "    error_data: List[Dict],\n",
    "    batch_size: int = 5,\n",
    "    sample_size: int = 100,\n",
    "    delay: float = 1.0\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses LLM to explain POS tagging errors.\n",
    "    \n",
    "    Args:\n",
    "        client: The LLM API client\n",
    "        error_data: List of detailed error data dictionaries\n",
    "        batch_size: Number of errors to process in each batch\n",
    "        sample_size: Maximum number of errors to sample\n",
    "        delay: Seconds to wait between API calls\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with error explanations\n",
    "    \"\"\"\n",
    "    # Sample errors if there are too many\n",
    "    if len(error_data) > sample_size:\n",
    "        sampled_errors = random.sample(error_data, sample_size)\n",
    "        print(f\"Sampled {sample_size} errors from {len(error_data)} total errors\")\n",
    "    else:\n",
    "        sampled_errors = error_data\n",
    "        \n",
    "    all_explanations = []\n",
    "    \n",
    "    for i in range(0, len(sampled_errors), batch_size):\n",
    "        batch = sampled_errors[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(sampled_errors) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Construct the prompt for this batch\n",
    "        prompt = \"\"\"\n",
    "        You are a computational linguist specializing in part-of-speech tagging using Universal Dependencies (UD).\n",
    "        \n",
    "        I will provide you with words that were incorrectly tagged by a POS tagger. \n",
    "        For each error, analyze why the error occurred and assign it to a specific category.\n",
    "        \n",
    "        For each error, provide:\n",
    "        1. A 1-3 sentence explanation of why this specific error likely occurred\n",
    "        2. A category name that captures the linguistic phenomenon behind the error\n",
    "        \n",
    "        Some possible error categories include:\n",
    "        - Ambiguity (ADJ/NOUN)\n",
    "        - Proper Noun vs Common Noun\n",
    "        - Function Word Misclassification\n",
    "        - Numeral/Ordinal Ambiguity\n",
    "        - Capitalization Issues\n",
    "        - Punctuation vs Symbol\n",
    "        - Auxiliary vs Main Verb Confusion\n",
    "        - Preposition/Adverb/Particle Ambiguity\n",
    "        - Contextual Dependency\n",
    "        \n",
    "        You may create new category names if needed, but be specific and consistent.\n",
    "        \n",
    "        Return your analysis as a structured list of explanations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add errors to the prompt\n",
    "        for error in batch:\n",
    "            word = error[\"word\"]\n",
    "            correct_tag = error[\"correct_tag\"]\n",
    "            predicted_tag = error[\"predicted_tag\"]\n",
    "            context = error[\"sentence_context\"]\n",
    "            \n",
    "            prompt += f\"\"\"\n",
    "\n",
    "            ERROR:\n",
    "            Word: {word}\n",
    "            Correct tag: {correct_tag}\n",
    "            Predicted tag: {predicted_tag}\n",
    "            Context: {context}\n",
    "            \"\"\"\n",
    "        \n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",  \n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                response_format=BatchErrorExplanation,\n",
    "            )\n",
    "            \n",
    "            batch_explanations = completion.choices[0].message.parsed.explanations\n",
    "            all_explanations.extend([exp.dict() for exp in batch_explanations])\n",
    "            \n",
    "            # Respect API rate limits\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            print(\"Will try again with individual errors\")\n",
    "            \n",
    "            # Fall back to processing individual errors\n",
    "            for error in batch:\n",
    "                individual_prompt = prompt + f\"\"\"\n",
    "                \n",
    "                ERROR:\n",
    "                Word: {error[\"word\"]}\n",
    "                Correct tag: {error[\"correct_tag\"]}\n",
    "                Predicted tag: {error[\"predicted_tag\"]}\n",
    "                Context: {error[\"sentence_context\"]}\n",
    "                \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    single_completion = client.beta.chat.completions.parse(\n",
    "                        model=\"grok-3\",\n",
    "                        messages=[{\"role\": \"system\", \"content\": individual_prompt}],\n",
    "                        response_format=ErrorExplanation,\n",
    "                    )\n",
    "                    \n",
    "                    explanation = single_completion.choices[0].message.parsed\n",
    "                    all_explanations.append(explanation.dict())\n",
    "                    time.sleep(delay)\n",
    "                except Exception as inner_e:\n",
    "                    print(f\"Error processing individual error: {inner_e}\")\n",
    "    \n",
    "    return all_explanations\n",
    "\n",
    "def analyze_error_categories(explanations):\n",
    "    \"\"\"Analyze the error explanations and visualize categories.\"\"\"\n",
    "    # Extract categories\n",
    "    categories = [expl[\"category\"] for expl in explanations]\n",
    "    category_counter = Counter(categories)\n",
    "    \n",
    "    print(\"\\nError Categories:\")\n",
    "    for category, count in category_counter.most_common():\n",
    "        print(f\"{category}: {count}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    categories, counts = zip(*category_counter.most_common())\n",
    "    plt.barh(categories, counts, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Number of Occurrences\")\n",
    "    plt.title(\"POS Tagging Error Categories\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.gca().invert_yaxis()  # Most common at the top\n",
    "    plt.show()\n",
    "    \n",
    "    return category_counter\n",
    "\n",
    "def run_error_analysis(client, evaluation_results):\n",
    "    \"\"\"\n",
    "    Run the complete error analysis process.\n",
    "    \n",
    "    Args:\n",
    "        client: The LLM API client\n",
    "        evaluation_results: Results from evaluate_llm_tagger function\n",
    "        \n",
    "    Returns:\n",
    "        Error explanations and category analysis\n",
    "    \"\"\"\n",
    "    if not evaluation_results or 'detailed_error_data' not in evaluation_results:\n",
    "        print(\"No detailed error data available\")\n",
    "        return None\n",
    "    \n",
    "    explanations = explain_tagging_errors(\n",
    "        client,\n",
    "        evaluation_results['detailed_error_data'],\n",
    "        batch_size=5,\n",
    "        sample_size=50  \n",
    "    )\n",
    "    \n",
    "    category_counts = analyze_error_categories(explanations)\n",
    "    \n",
    "    print(\"\\nError Explanations:\")\n",
    "    print(json.dumps(explanations, indent=2))\n",
    "    \n",
    "    return {\n",
    "        'explanations': explanations,\n",
    "        'category_counts': category_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run error analysis on the evaluation results\n",
    "analysis_results = run_error_analysis(client, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, RootModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SyntheticSentence(BaseModel):\n",
    "    sentence: List[str]\n",
    "    tags: List[str]\n",
    "    categories: List[str] \n",
    "\n",
    "class SyntheticBatch(RootModel[List[SyntheticSentence]]):\n",
    "    pass\n",
    "\n",
    "# Build prompt dynamically\n",
    "def build_synthetic_prompt(categories: List[str]) -> str:\n",
    "    joined = \", \".join(categories)\n",
    "    return f\"\"\"\n",
    "You are an NLP expert generating training data for a POS tagger.\n",
    "Generate 2 English sentences that demonstrate POS tagging challenges involving: {joined}.\n",
    "\n",
    "For each sentence, return:\n",
    "- A list of tokens\n",
    "- Their corresponding UD POS tags\n",
    "\n",
    "Use only the Universal Dependencies POS tagset for tagging (ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X). Each token must receive exactly one tag from this set — no combinations, no ambiguity markers.\n",
    "\n",
    "Return a valid JSON list like:\n",
    "[\n",
    "  {{\n",
    "    \"sentence\": [...],\n",
    "    \"tags\": [...]\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Only return JSON. Do not explain.\n",
    "\"\"\"\n",
    "\n",
    "# Generate one batch with retry and category attachment\n",
    "def generate_batch(categories: List[str], max_retries=3) -> List[SyntheticSentence]:\n",
    "    prompt = build_synthetic_prompt(categories)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                response_format=SyntheticBatch,\n",
    "            )\n",
    "            results = completion.choices[0].message.parsed.root\n",
    "            for r in results:\n",
    "                r.categories = categories\n",
    "            return [r for r in results if len(r.sentence) == len(r.tags)]\n",
    "        except Exception as e:\n",
    "            wait = 2 + random.uniform(0, 2)\n",
    "            print(f\"[Retry {attempt+1}] Error: {e} — waiting {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    print(\" Failed after retries.\")\n",
    "    return []\n",
    "\n",
    "# Run multiple batches using weighted sampling from category counts\n",
    "def run_parallel_generation_from_counter(category_counter, n_batches: int = 200, n_threads: int = 5) -> List[SyntheticSentence]:\n",
    "    category_pool = list(category_counter.elements()) \n",
    "    unique_categories = sorted(set(category_pool))\n",
    "\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(generate_batch, random.sample(category_pool, k=3))\n",
    "            for _ in range(n_batches)\n",
    "        ]\n",
    "        for future in tqdm(as_completed(futures), total=n_batches):\n",
    "            batch = future.result()\n",
    "            all_results.extend(batch)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    category_counter = analysis_results[\"category_counts\"]  \n",
    "\n",
    "    n_batches = 100   # ≈ 200 sentences\n",
    "    n_threads = 5\n",
    "\n",
    "    results = run_parallel_generation_from_counter(category_counter, n_batches=n_batches, n_threads=n_threads)\n",
    "\n",
    "    print(f\"\\n Done! Generated {len(results)} synthetic sentences.\\n\")\n",
    "\n",
    "    for i, r in enumerate(results[:5]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(\"Sentence:  \", \" \".join(r.sentence))\n",
    "        print(\"Tags:      \", r.tags)\n",
    "        print(\"Categories:\", r.categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_synthetic_to_tagged_sentences(synthetic: List[SyntheticSentence]):\n",
    "    return [list(zip(s.sentence, s.tags)) for s in synthetic]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_batches = 100\n",
    "    n_threads = 5\n",
    "    # Convert to training format and combine\n",
    "    synthetic_tagged = convert_synthetic_to_tagged_sentences(results)\n",
    "    train_augmented = train_sentences + synthetic_tagged\n",
    "    random.shuffle(train_augmented)\n",
    "\n",
    "    print(\"Vectorizing data...\")\n",
    "    X_train_synth, y_train_synth, X_val_synth, y_val_synth, X_test_synth, y_test_synth, vectorizer_synth = vectorize(train_augmented, val_sentences, test_sentences)\n",
    "\n",
    "    # Train the LR tagger\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    t_ini = datetime.datetime.now()\n",
    "    clf_synth = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "    clf_synth.fit(X_train_synth, y_train_synth)\n",
    "    t_fin = datetime.datetime.now()\n",
    "    print(f\"Training completed in {(t_fin - t_ini).total_seconds():.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred_orig = clf.predict(X_test)\n",
    "y_pred_synth = clf_synth.predict(X_test_synth)\n",
    "\n",
    "# Track positions where the original was wrong but the synthetic fixed it, and vice versa\n",
    "fixed_by_synth = []\n",
    "regressed_by_synth = []\n",
    "unchanged_errors = []\n",
    "\n",
    "for i, (true, pred_orig, pred_synth) in enumerate(zip(y_test_synth, y_pred_orig, y_pred_synth)):\n",
    "    if pred_orig != true and pred_synth == true:\n",
    "        fixed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig == true and pred_synth != true:\n",
    "        regressed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig != true and pred_synth != true:\n",
    "        unchanged_errors.append((i, true, pred_orig, pred_synth))\n",
    "\n",
    "print(\"Contrastive Error Analysis\")\n",
    "print(f\"Fixed errors by synthetic model: {len(fixed_by_synth)}\")\n",
    "print(f\"New errors introduced by synthetic model: {len(regressed_by_synth)}\")\n",
    "print(f\"Unchanged errors (both models wrong): {len(unchanged_errors)}\")\n",
    "\n",
    "print(\"\\n--- Original Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_orig, digits=3))\n",
    "\n",
    "print(\"\\n--- Synthetic-Augmented Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_synth, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_correct_tokenized_indices(lr_hard_sentences, metrics):\n",
    "    \"\"\"\n",
    "    Collects indices of sentences that were correctly tagged by the LLM in tokenized form.\n",
    "    \n",
    "    Args:\n",
    "        lr_hard_sentences: List of (sentence, errors, lr_tags, idx) tuples\n",
    "        metrics: Dictionary with metrics from evaluate_llm_tagger\n",
    "        \n",
    "    Returns:\n",
    "        List of indices in test_original\n",
    "    \"\"\"\n",
    "    llm_hard_sentences = metrics.get('hard_sentences', [])\n",
    "    \n",
    "    # Create a set of sentences with errors in LLM tagger\n",
    "    llm_error_sentences = {id(sent) for sent, _ in llm_hard_sentences}\n",
    "    \n",
    "    # Collect indices of sentences without errors in LLM tagger\n",
    "    correct_indices = []\n",
    "    for sent, _, _, idx in lr_hard_sentences:\n",
    "        if id(sent) not in llm_error_sentences:\n",
    "            correct_indices.append(idx)\n",
    "    \n",
    "    return correct_indices\n",
    "\n",
    "\n",
    "def collect_failed_sentences(test_sentences, test_original, correct_indices, batch_size=5):\n",
    "    \"\"\"\n",
    "    Tests the LLM tagger on original sentences and collects those that fail.\n",
    "    \"\"\"\n",
    "    original_sentences = [test_original[idx] for idx in correct_indices]\n",
    "    tokenized_sentences = [test_sentences[idx] for idx in correct_indices]\n",
    "    \n",
    "    results_original = batch_tag_sentences_ud(original_sentences, batch_size=batch_size)\n",
    "    \n",
    "    total_sentences = len(original_sentences)\n",
    "    segmentation_errors = []\n",
    "    tagging_errors = []\n",
    "    \n",
    "    for i, (orig_sent, token_sent, result, idx) in enumerate(zip(original_sentences, tokenized_sentences, results_original, correct_indices)):\n",
    "        if result is None or not hasattr(result, 'sentences') or not result.sentences:\n",
    "            # Count as a segmentation error if we couldn't process\n",
    "            segmentation_errors.append((idx, orig_sent, token_sent))\n",
    "            continue\n",
    "            \n",
    "        pred_tokens = [token.text for token in result.sentences[0].tokens]\n",
    "        pred_tags = [token.pos_tag.value if hasattr(token.pos_tag, 'value') else str(token.pos_tag) \n",
    "                    for token in result.sentences[0].tokens]\n",
    "        true_tokens = [word for word, _ in token_sent]\n",
    "        true_tags = [tag for _, tag in token_sent]\n",
    "        \n",
    "        # Check if segmentation matches\n",
    "        if len(pred_tokens) != len(true_tokens):\n",
    "            segmentation_errors.append((idx, orig_sent, token_sent))\n",
    "            continue\n",
    "            \n",
    "        # Check if tagging is correct\n",
    "        tag_error = False\n",
    "        for j, (pt, tt) in enumerate(zip(pred_tags, true_tags)):\n",
    "            if str(pt) != str(tt):\n",
    "                tag_error = True\n",
    "                break\n",
    "                \n",
    "        if tag_error:\n",
    "            tagging_errors.append((idx, orig_sent, token_sent))\n",
    "    \n",
    "    total_errors = len(segmentation_errors) + len(tagging_errors)\n",
    "    success_count = total_sentences - total_errors  \n",
    "    \n",
    "    print(f\"Total sentences that succeeded with tokenized version: {total_sentences}\")\n",
    "    print(f\"Of these, failed with original version: {total_errors} ({(total_errors/total_sentences)*100:.2f}%)\")\n",
    "    print(f\"  - Segmentation errors: {len(segmentation_errors)} ({(len(segmentation_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\"  - Tagging errors: {len(tagging_errors)} ({(len(tagging_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\"Succeeded with both versions: {success_count} ({(success_count/total_sentences)*100:.2f}%)\")\n",
    "    \n",
    "    if len(segmentation_errors) > 0:\n",
    "        print(\"\\nExample segmentation error:\")\n",
    "        idx, orig, tagged = segmentation_errors[0]\n",
    "        print(f\"Original: {orig}\")\n",
    "        print(f\"Expected tokenization: {[word for word, _ in tagged]}\")\n",
    "        \n",
    "    if len(tagging_errors) > 0:\n",
    "        print(\"\\nExample tagging error:\")\n",
    "        idx, orig, tagged = tagging_errors[0]\n",
    "        print(f\"Original: {orig}\")\n",
    "        print(f\"Tokenization correct but tags were wrong\")\n",
    "    \n",
    "    failed_sentences = segmentation_errors + tagging_errors\n",
    "    \n",
    "    return {\n",
    "        'total_sentences': total_sentences,\n",
    "        'total_errors': total_errors,\n",
    "        'segmentation_errors': segmentation_errors,\n",
    "        'tagging_errors': tagging_errors,\n",
    "        'success_count': success_count,\n",
    "        'failed_sentences': failed_sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = collect_correct_tokenized_indices(lr_hard_sentences, metrics)\n",
    "failed_results = collect_failed_sentences(test_sentences, test_original, correct_indices, batch_size=5)\n",
    "\n",
    "# Get all failed sentences\n",
    "failed_sentences = failed_results['failed_sentences']\n",
    "print(f\"Total failed sentences: {len(failed_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedSentence(BaseModel):\n",
    "    tokens: List[str] = Field(description=\"List of tokens produced by segmentation\")\n",
    "\n",
    "class SegmentationResult(BaseModel):\n",
    "    \"\"\"Represents a list of tokenized sentences.\"\"\"\n",
    "    sentences: List[TokenizedSentence] = Field(description=\"A list of tokenized sentences\")\n",
    "\n",
    "\n",
    "def segment_sentences_ud(text: str, examples: List[Dict[str, List[str]]] = None) -> Optional[SegmentationResult]:\n",
    "    \"\"\"\n",
    "    Segments a sentence into UD-style tokens using Grok.\n",
    "    \n",
    "    Args:\n",
    "        text: The sentence to segment\n",
    "        examples: Optional list of example dictionaries. Each dict should have keys:\n",
    "                 'original': original sentence string\n",
    "                 'tokenized': list of tokenized words\n",
    "    \n",
    "    Returns:\n",
    "        A SegmentationResult object containing the tokenized sentences, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Build the prompt - different for zero-shot and few-shot\n",
    "    if examples is None or len(examples) == 0:\n",
    "        # Zero-shot prompt\n",
    "        prompt = f\"\"\"You are a specialized tokenizer following Universal Dependencies (UD) guidelines for English.\n",
    "        \n",
    "        Your task is to segment the provided English sentence according to UD tokenization rules, which include:\n",
    "        \n",
    "        1. Split on whitespace and punctuation (except in URLs, numbers, abbreviations)\n",
    "        2. Split contractions: \"don't\" → [\"do\", \"n't\"], \"it's\" → [\"it\", \"'s\"]\n",
    "        3. Separate possessive markers: \"Elena's\" → [\"Elena\", \"'s\"]\n",
    "        4. Split hyphenated compounds: \"search-engine\" → [\"search\", \"-\", \"engine\"]\n",
    "        5. Keep punctuation as separate tokens\n",
    "        6. Preserve numbers with internal periods/commas (e.g., 3.14, 1,000)\n",
    "        7. Do not merge words except for contractions/clitics\n",
    "        8. Acronyms (FBI, U.S.) should stay as single tokens\n",
    "        \n",
    "        Return only the tokenized result in a structured format.\n",
    "        \n",
    "        Sentence to tokenize:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        \n",
    "        system_message = {\"role\": \"system\", \"content\": prompt}\n",
    "        user_message = None\n",
    "    else:\n",
    "        # Few-shot prompt\n",
    "        system_prompt = \"\"\"You are a specialized tokenizer following Universal Dependencies (UD) guidelines for English.\n",
    "        \n",
    "        Your task is to segment the provided English sentence according to UD tokenization rules, which include:\n",
    "        \n",
    "        1. Split on whitespace and punctuation (except in URLs, numbers, abbreviations)\n",
    "        2. Split contractions: \"don't\" → [\"do\", \"n't\"], \"it's\" → [\"it\", \"'s\"]\n",
    "        3. Separate possessive markers: \"Elena's\" → [\"Elena\", \"'s\"]\n",
    "        4. Split hyphenated compounds: \"search-engine\" → [\"search\", \"-\", \"engine\"]\n",
    "        5. Keep punctuation as separate tokens\n",
    "        6. Preserve numbers with internal periods/commas (e.g., 3.14, 1,000)\n",
    "        7. Do not merge words except for contractions/clitics\n",
    "        8. Acronyms (FBI, U.S.) should stay as single tokens\n",
    "        \n",
    "        Look carefully at the examples, follow the same segmentation patterns, and return the results in the desired JSON format.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert examples to text for few-shot prompting\n",
    "        examples_text = \"\"\n",
    "        for i, example in enumerate(examples):\n",
    "            original = example.get('original', '')\n",
    "            tokenized = example.get('tokenized', [])\n",
    "            examples_text += f\"Example {i+1}:\\n\"\n",
    "            examples_text += f\"Original: {original}\\n\"\n",
    "            examples_text += f\"Tokenized: {tokenized}\\n\\n\"\n",
    "        \n",
    "        examples_text += f\"Now tokenize this sentence:\\n{text}\"\n",
    "        \n",
    "        system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "        user_message = {\"role\": \"user\", \"content\": examples_text}\n",
    "\n",
    "    try:\n",
    "        messages = [system_message]\n",
    "        if user_message:\n",
    "            messages.append(user_message)\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"grok-3\",\n",
    "            messages=messages,\n",
    "            response_format=SegmentationResult,\n",
    "        )\n",
    "        res = completion.choices[0].message.parsed\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"Error in segmentation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_segment(text: str, examples: List[Dict[str, List[str]]] = None) -> Optional[SegmentationResult]:\n",
    "    return segment_sentences_ud(text, examples)\n",
    "\n",
    "def batch_segment_sentences_ud(sentences: List[str], \n",
    "                               batch_size: int = 15, \n",
    "                               examples: List[Dict[str, List[str]]] = None) -> List[SegmentationResult]:\n",
    "    \"\"\"\n",
    "    Process sentences in batches with optional few-shot examples\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences to segment\n",
    "        batch_size: Number of sentences to process in each batch\n",
    "        examples: Optional list of example dictionaries for few-shot prompting\n",
    "    \n",
    "    Returns:\n",
    "        List of SegmentationResult objects\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        # Convert batch to JSON string\n",
    "        batch_json = json.dumps(batch)\n",
    "        batches.append(batch_json)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches with parallel workers...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_segment, \n",
    "                                           batch_json,\n",
    "                                           examples): i \n",
    "                          for i, batch_json in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_segmenter(gold_sentences, pred_sentences):\n",
    "    \"\"\"\n",
    "    Evaluates the segmenter's performance\n",
    "    \n",
    "    Args:\n",
    "        gold_sentences: List of lists containing gold tokenization \n",
    "        pred_sentences: List of lists containing predicted tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    total_sentences = len(gold_sentences)\n",
    "    correct_sentences = 0\n",
    "    tokenization_errors = []\n",
    "    \n",
    "    for i, (gold, pred) in enumerate(zip(gold_sentences, pred_sentences)):\n",
    "        if gold == pred:\n",
    "            correct_sentences += 1\n",
    "        else:\n",
    "            tokenization_errors.append({\n",
    "                'index': i,\n",
    "                'gold': gold,\n",
    "                'pred': pred\n",
    "            })\n",
    "    \n",
    "    accuracy = correct_sentences / total_sentences if total_sentences > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'correct_sentences': correct_sentences,\n",
    "        'total_sentences': total_sentences,\n",
    "        'error_count': total_sentences - correct_sentences,\n",
    "        'errors': tokenization_errors\n",
    "    }\n",
    "\n",
    "\n",
    "# Default examples for few-shot prompting\n",
    "default_examples = [\n",
    "    {\n",
    "        'original': \"John's dog doesn't like the cat-food in the store.\",\n",
    "        'tokenized': [\"John\", \"'s\", \"dog\", \"does\", \"n't\", \"like\", \"the\", \"cat\", \"-\", \"food\", \"in\", \"the\", \"store\", \".\"]\n",
    "    },\n",
    "    {\n",
    "        'original': \"The search-engine (and e-mail) functions aren't ready yet!\",\n",
    "        'tokenized': [\"The\", \"search\", \"-\", \"engine\", \"(\", \"and\", \"e\", \"-\", \"mail\", \")\", \"functions\", \"are\", \"n't\", \"ready\", \"yet\", \"!\"]\n",
    "    },\n",
    "    {\n",
    "        'original': \"We'll be visiting the U.S. next month for the conference.\",\n",
    "        'tokenized': [\"We\", \"'ll\", \"be\", \"visiting\", \"the\", \"U.S.\", \"next\", \"month\", \"for\", \"the\", \"conference\", \".\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def test_segmenter_on_failed_sentences(failed_sentences, examples=None, batch_size=5):\n",
    "    \"\"\"\n",
    "    Tests the segmenter on sentences that failed with original text but succeeded with tokenized.\n",
    "    \n",
    "    Args:\n",
    "        failed_sentences: List of (idx, original, tokenized) tuples\n",
    "        examples: Optional list of example dictionaries for few-shot prompting\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    original_texts = [f[1] for f in failed_sentences]\n",
    "    gold_tokenized = [untag(f[2]) for f in failed_sentences] \n",
    "    \n",
    "    results = batch_segment_sentences_ud(original_texts, batch_size=batch_size, examples=examples)\n",
    "    \n",
    "    pred_tokenized = []\n",
    "    for result in results:\n",
    "        if result and hasattr(result, 'sentences'):\n",
    "            for sentence in result.sentences:\n",
    "                pred_tokenized.append(sentence.tokens)\n",
    "    \n",
    "    # In case of any length mismatch, ensure alignment\n",
    "    if len(pred_tokenized) != len(gold_tokenized):\n",
    "        print(f\"Warning: Prediction count ({len(pred_tokenized)}) doesn't match gold count ({len(gold_tokenized)})\")\n",
    "        # Truncate to the shorter length to avoid errors\n",
    "        length = min(len(pred_tokenized), len(gold_tokenized))\n",
    "        pred_tokenized = pred_tokenized[:length]\n",
    "        gold_tokenized = gold_tokenized[:length]\n",
    "    \n",
    "    metrics = eval_segmenter(gold_tokenized, pred_tokenized)\n",
    "    \n",
    "    print(f\"Segmentation Evaluation Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Correct sentences: {metrics['correct_sentences']} out of {metrics['total_sentences']}\")\n",
    "    print(f\"Error count: {metrics['error_count']}\")\n",
    "    \n",
    "    if len(metrics['errors']) > 0:\n",
    "        print(\"\\nError Examples:\")\n",
    "        for i, error in enumerate(metrics['errors'][:3]): \n",
    "            idx = error['index']\n",
    "            print(f\"Original: {original_texts[idx]}\")\n",
    "            print(f\"Gold: {error['gold']}\")\n",
    "            print(f\"Pred: {error['pred']}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "zero_shot_metrics = test_segmenter_on_failed_sentences(failed_sentences)\n",
    "few_shot_metrics = test_segmenter_on_failed_sentences(failed_sentences, examples=default_examples)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Zero-shot Accuracy: {zero_shot_metrics['accuracy']:.4f}\")\n",
    "print(f\"Few-shot Accuracy: {few_shot_metrics['accuracy']:.4f}\")\n",
    "print(f\"Improvement: {(few_shot_metrics['accuracy'] - zero_shot_metrics['accuracy']) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_segment_and_tag(sentences, segmenter_examples=None, batch_size=5):\n",
    "    \"\"\"\n",
    "    Pipeline approach: first segment, then tag\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of original sentences\n",
    "        segmenter_examples: Optional examples for few-shot segmentation\n",
    "        batch_size: Batch size for API calls\n",
    "        \n",
    "    Returns:\n",
    "        Segmented and tagged sentences\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Segmentation\")\n",
    "    \n",
    "    segmentation_results = batch_segment_sentences_ud(\n",
    "        sentences, \n",
    "        batch_size=batch_size,\n",
    "        examples=segmenter_examples\n",
    "    )\n",
    "    \n",
    "    # Extract tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    for result in segmentation_results:\n",
    "        if result and hasattr(result, 'sentences'):\n",
    "            for sentence in result.sentences:\n",
    "                tokenized_sentences.append(sentence.tokens)\n",
    "        \n",
    "    if len(tokenized_sentences) == 0:\n",
    "        print(\"No sentences were successfully segmented. Aborting pipeline.\")\n",
    "        return []\n",
    "    \n",
    "    print(\"Step 2: Tagging pre-tokenized sentences\")\n",
    "    \n",
    "    # Step 2: Run the tagger on tokenized sentences\n",
    "    tagging_results = batch_tag_pretokenized_sentences(\n",
    "        tokenized_sentences,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return tagging_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_approach(failed_sentences, segmenter_examples=None, batch_size=5):\n",
    "    \"\"\"\n",
    "    Tests the pipeline approach on failed sentences and reports metrics\n",
    "    \n",
    "    Args:\n",
    "        failed_sentences: List of (idx, original, tokenized) tuples\n",
    "        segmenter_examples: Optional examples for few-shot segmentation\n",
    "        batch_size: Batch size for API calls\n",
    "    \"\"\"\n",
    "    original_sentences = [f[1] for f in failed_sentences]\n",
    "    gold_sentences = [f[2] for f in failed_sentences]\n",
    "    \n",
    "    pipeline_results = pipeline_segment_and_tag(\n",
    "        original_sentences,\n",
    "        segmenter_examples=segmenter_examples,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    pred_tokens_all = []\n",
    "    pred_tags_all = []\n",
    "    \n",
    "    for result in pipeline_results:\n",
    "        if result and hasattr(result, 'sentences'):\n",
    "            for sentence in result.sentences:\n",
    "                tokens = [token.text for token in sentence.tokens]\n",
    "                tags = [token.pos_tag.value if hasattr(token.pos_tag, 'value') else str(token.pos_tag) \n",
    "                       for token in sentence.tokens]\n",
    "                pred_tokens_all.append(tokens)\n",
    "                pred_tags_all.append(tags)\n",
    "    \n",
    "    total_sentences = len(original_sentences)\n",
    "    segmentation_errors = []\n",
    "    tagging_errors = []\n",
    "    \n",
    "    # Handle case where we have fewer predictions than expected\n",
    "    if len(pred_tokens_all) < total_sentences:\n",
    "        print(f\"Warning: Got {len(pred_tokens_all)} predictions but expected {total_sentences}\")\n",
    "        for i in range(len(pred_tokens_all), total_sentences):\n",
    "            segmentation_errors.append(i)\n",
    "        num_to_check = len(pred_tokens_all)\n",
    "    else:\n",
    "        num_to_check = total_sentences\n",
    "    \n",
    "    for i in range(num_to_check):\n",
    "        gold_tokens = [word for word, _ in gold_sentences[i]]\n",
    "        gold_tags = [tag for _, tag in gold_sentences[i]]\n",
    "        \n",
    "        # Check segmentation\n",
    "        if len(pred_tokens_all[i]) != len(gold_tokens) or pred_tokens_all[i] != gold_tokens:\n",
    "            segmentation_errors.append(i)\n",
    "            continue\n",
    "            \n",
    "        # Check tagging\n",
    "        tag_error = False\n",
    "        for j, (pred_tag, gold_tag) in enumerate(zip(pred_tags_all[i], gold_tags)):\n",
    "            if str(pred_tag) != str(gold_tag):\n",
    "                tag_error = True\n",
    "                break\n",
    "                \n",
    "        if tag_error:\n",
    "            tagging_errors.append(i)\n",
    "    \n",
    "    total_errors = len(segmentation_errors) + len(tagging_errors)\n",
    "    success_count = total_sentences - total_errors\n",
    "    \n",
    "    # Print metrics in the exact requested format\n",
    "    print(f\"Total sentences that succeeded with tokenized version: {total_sentences}\")\n",
    "    print(f\"Of these, failed with pipeline approach: {total_errors} ({(total_errors/total_sentences)*100:.2f}%)\")\n",
    "    print(f\" - Segmentation errors: {len(segmentation_errors)} ({(len(segmentation_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\" - Tagging errors: {len(tagging_errors)} ({(len(tagging_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\"Succeeded with pipeline approach: {success_count} ({(success_count/total_sentences)*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_sentences': total_sentences,\n",
    "        'total_errors': total_errors,\n",
    "        'segmentation_errors': segmentation_errors,\n",
    "        'tagging_errors': tagging_errors,\n",
    "        'success_count': success_count\n",
    "    }\n",
    "\n",
    "pipeline_metrics = test_pipeline_approach(\n",
    "    failed_sentences, \n",
    "    segmenter_examples=default_examples,\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_on_correct_indices(test_sentences, test_original, correct_indices, segmenter_examples=None, batch_size=5):\n",
    "    \"\"\"\n",
    "    Tests the pipeline approach on sentences that were correctly tagged by the LLM in tokenized form.\n",
    "    \n",
    "    Args:\n",
    "        test_sentences: List of tokenized sentences from the dataset\n",
    "        test_original: List of original sentences (non-tokenized)\n",
    "        correct_indices: List of indices of sentences correctly tagged by the LLM\n",
    "        segmenter_examples: Optional examples for few-shot segmentation\n",
    "        batch_size: Batch size for API calls\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Extract sentences using the correct indices\n",
    "    original_sentences = [test_original[idx] for idx in correct_indices]\n",
    "    tokenized_sentences = [test_sentences[idx] for idx in correct_indices]\n",
    "    \n",
    "    print(f\"Testing pipeline approach on {len(original_sentences)} sentences that were correctly tagged with tokenized input\")\n",
    "    \n",
    "    # Run the pipeline approach\n",
    "    pipeline_results = pipeline_segment_and_tag(\n",
    "        original_sentences,\n",
    "        segmenter_examples=segmenter_examples,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Extract predictions\n",
    "    pred_tokens_all = []\n",
    "    pred_tags_all = []\n",
    "    \n",
    "    for result in pipeline_results:\n",
    "        if result and hasattr(result, 'sentences'):\n",
    "            for sentence in result.sentences:\n",
    "                tokens = [token.text for token in sentence.tokens]\n",
    "                tags = [token.pos_tag.value if hasattr(token.pos_tag, 'value') else str(token.pos_tag) \n",
    "                       for token in sentence.tokens]\n",
    "                pred_tokens_all.append(tokens)\n",
    "                pred_tags_all.append(tags)\n",
    "    \n",
    "    # Count errors\n",
    "    total_sentences = len(original_sentences)\n",
    "    segmentation_errors = []\n",
    "    tagging_errors = []\n",
    "    \n",
    "    # Handle case where we have fewer predictions than expected\n",
    "    if len(pred_tokens_all) < total_sentences:\n",
    "        print(f\"Warning: Got {len(pred_tokens_all)} predictions but expected {total_sentences}\")\n",
    "        # Count missing predictions as segmentation errors\n",
    "        for i in range(len(pred_tokens_all), total_sentences):\n",
    "            segmentation_errors.append(i)\n",
    "        num_to_check = len(pred_tokens_all)\n",
    "    else:\n",
    "        num_to_check = total_sentences\n",
    "    \n",
    "    # Check each sentence\n",
    "    for i in range(num_to_check):\n",
    "        gold_tokens = [word for word, _ in tokenized_sentences[i]]\n",
    "        gold_tags = [tag for _, tag in tokenized_sentences[i]]\n",
    "        \n",
    "        # Check segmentation\n",
    "        if len(pred_tokens_all[i]) != len(gold_tokens) or pred_tokens_all[i] != gold_tokens:\n",
    "            segmentation_errors.append(i)\n",
    "            continue\n",
    "            \n",
    "        # Check tagging\n",
    "        tag_error = False\n",
    "        for j, (pred_tag, gold_tag) in enumerate(zip(pred_tags_all[i], gold_tags)):\n",
    "            if str(pred_tag) != str(gold_tag):\n",
    "                tag_error = True\n",
    "                break\n",
    "                \n",
    "        if tag_error:\n",
    "            tagging_errors.append(i)\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    total_errors = len(segmentation_errors) + len(tagging_errors)\n",
    "    success_count = total_sentences - total_errors\n",
    "    \n",
    "    # Print metrics in the exact requested format\n",
    "    print(f\"Total sentences that succeeded with tokenized version: {total_sentences}\")\n",
    "    print(f\"Of these, failed with pipeline approach: {total_errors} ({(total_errors/total_sentences)*100:.2f}%)\")\n",
    "    print(f\" - Segmentation errors: {len(segmentation_errors)} ({(len(segmentation_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\" - Tagging errors: {len(tagging_errors)} ({(len(tagging_errors)/total_sentences)*100:.2f}%)\")\n",
    "    print(f\"Succeeded with pipeline approach: {success_count} ({(success_count/total_sentences)*100:.2f}%)\")\n",
    "    \n",
    "    # Optionally, show some examples of errors\n",
    "    if segmentation_errors:\n",
    "        print(\"\\nExample segmentation error:\")\n",
    "        idx = segmentation_errors[0]\n",
    "        print(f\"Original: {original_sentences[idx]}\")\n",
    "        print(f\"Expected tokenization: {[word for word, _ in tokenized_sentences[idx]]}\")\n",
    "        if idx < len(pred_tokens_all):\n",
    "            print(f\"Pipeline tokenization: {pred_tokens_all[idx]}\")\n",
    "    \n",
    "    if tagging_errors:\n",
    "        print(\"\\nExample tagging error:\")\n",
    "        idx = tagging_errors[0] \n",
    "        print(f\"Original: {original_sentences[idx]}\")\n",
    "        print(f\"Tokenization correct but tags were wrong\")\n",
    "        if idx < len(pred_tags_all):\n",
    "            gold_tags = [tag for _, tag in tokenized_sentences[idx]]\n",
    "            print(f\"Expected tags: {gold_tags}\")\n",
    "            print(f\"Pipeline tags: {pred_tags_all[idx]}\")\n",
    "    \n",
    "    return {\n",
    "        'total_sentences': total_sentences,\n",
    "        'total_errors': total_errors,\n",
    "        'segmentation_errors': segmentation_errors,\n",
    "        'tagging_errors': tagging_errors,\n",
    "        'success_count': success_count\n",
    "    }\n",
    "\n",
    "# Then test the pipeline approach on those sentences\n",
    "pipeline_metrics = test_pipeline_on_correct_indices(\n",
    "    test_sentences,\n",
    "    test_original,\n",
    "    correct_indices,\n",
    "    segmenter_examples=default_examples,\n",
    "    batch_size=5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
