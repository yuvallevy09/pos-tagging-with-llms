{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger Trained on the UD Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/soutsios/pos_tagger_mlp/blob/master/pos_tagger_mlp.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Functions\n",
    "\n",
    "These functions are useful to visualize the training dynamics of the learning algorithm and the confusion matrix of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll, nltk, datetime, warnings\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(f1,\n",
    "                          cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          i=1):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_ENGLISH_TRAIN = '../UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "UD_ENGLISH_DEV = '../UD_English-EWT/en_ewt-ud-dev.conllu'\n",
    "UD_ENGLISH_TEST = '../UD_English-EWT/en_ewt-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    original_sentences=[]\n",
    "    t=0\n",
    "    for sentence in data:\n",
    "        original_sentences.append(sentence.text)\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos:\n",
    "                t+=1\n",
    "                tagged_sentence.append((token.form if token.form else '*None*', token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences, original_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train, development and test set in the appropriate tagged format, tuple (word, pos-tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_original = read_conllu(UD_ENGLISH_TRAIN)\n",
    "val_sentences, val_original = read_conllu(UD_ENGLISH_DEV)\n",
    "test_sentences, test_original = read_conllu(UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important observation: how many terms are in validation set and not found in train set? (This estimates the Out-of-vocabulary rate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = build_vocab(train_sentences)\n",
    "list_2 = build_vocab(val_sentences)\n",
    "diff_list = [item for item in list_2 if item not in list_1]\n",
    "print('Number of terms not found in train set:',len(diff_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We use the \"classical\" machine learning approach: we will train a token classifier model. The classifier gets as input a feature vector describing each token in the sentence. We decide a priori which features are informative to make the tagging decision. In this case, we use a combination of \"word shape\" features which approximate morphological knowledge. We naturally also include lexical information (the token form itself), and some form of \"syntactic knowledge\" by adding reference to the previous and next word in each token feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'word_lower': sentence[index].lower(),\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1].lower(),\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1].lower(),\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scikit-learn model, we model a dataset as a pair of two data structures:\n",
    "* The list of feature dictionaries X (one feature dictionary for each token)\n",
    "* The list of predicted label y (one tag for each token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index)])\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence represented as a list of tokens, return the list of feature dictionaries using our feature encoding method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([features_basic(sentence, index)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test untag()\n",
    "\n",
    "We use untag() to extract raw sentences from the annotated CoNLL dataset. This way we can reproduce a sentence without tags, submit it to the tagger and compare predictions to the gold tags that are provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untag(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply a generic machine learning algorithm (such as Logistic Regression), we need to encode the dataset into a vectorized format.\n",
    "\n",
    "We proceed in two steps: feature engineering and vectorization.\n",
    "\n",
    "For each token, we create a dictionary of features that depend on the sentence from which the token is extracted. \n",
    "These features include the word itself, the word before and the word after, letter suffixes and prefixes, etc.\n",
    "\n",
    "In the scikit-learn approach, before we can use a generic machine learning algorithm, we must then \"vectorize\" the feature dictionaries into vector encodings.\n",
    "For example, lexical features are encoded into one-hot vectors whose dimension is the size of the vocabulary.\n",
    "Note the difference between the method `fit_transform` of the vectorizer, which \"learns\" how to vectorize features, and `transform` which applies a learned vectorizer to feature dictionaries.  We use `fit_transform` on the training data, and `transform` on the other sections (validation and test).\n",
    "\n",
    "These vector representations are what is passed to the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, val, test):\n",
    "\n",
    "    print('Feature encoding method')\n",
    "    print('Vectorizing Dataset...')\n",
    "    print('Vectorizing train...')\n",
    "    X_train, y_train = transform_to_dataset(train)\n",
    "    v = DictVectorizer(sparse=True) \n",
    "    X_train = v.fit_transform([x[0] for x in X_train])\n",
    "    \n",
    "    print('Vectorizing validation...')\n",
    "    X_val, y_val = transform_to_dataset(val)\n",
    "    X_val = v.transform([x[0] for x in X_val])        \n",
    "    \n",
    "    print('Vectorizing test...')\n",
    "    X_test, y_test = transform_to_dataset(test)\n",
    "    X_test = v.transform([x[0] for x in X_test])\n",
    "    \n",
    "    print('Dataset vectorized.')\n",
    "    print('Train shape:', X_train.shape)\n",
    "    print('Validation shape:', X_val.shape)\n",
    "    print('Test shape:', X_test.shape)\n",
    "    \n",
    "    # Compress sparse matrices\n",
    "    X_train = X_train \n",
    "    X_val = X_val \n",
    "    X_test = X_test\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a \"real\" machine learning algorithm using scikit-learn, we will repeat the very simple statistical method we discussed in class.\n",
    "We train and evaluate the Baseline Unigram tagger to compare performance with the tagger we will train next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "unigram_tagger = nltk.UnigramTagger(train_sentences+val_sentences, backoff=default_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [item for sublist in tag_sequence(train_sentences+val_sentences) for item in sublist]\n",
    "y_test = [item for sublist in tag_sequence(test_sentences) for item in sublist]\n",
    "classes = sorted(list(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tag_list(apply_tagger(unigram_tagger, test_sentences))\n",
    "print(\"Accuracy: {0:.4f}\".format(unigram_tagger.accuracy(test_sentences)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, zero_division=1, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=1, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what errors the Baseline tagger makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tagger(tagged_sentence):\n",
    "    note = ''\n",
    "    for tup in list(zip(unigram_tagger.tag(untag(tagged_sentence)),untag_pos(tagged_sentence))):\n",
    "        if tup[0][1]!=tup[1]: note='<<--- Error!'\n",
    "        print(tup[0], tup[1], note)\n",
    "        note=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tagger(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes mistakes. Unsurprising given its simplistic approach and the small size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(f1_score(y_test, y_pred, average='macro'), cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move to a more serious machine learning model. We will train a Logistic Regression model using our feature extraction function based on our \"expertise\" in the domain.\n",
    "\n",
    "We first transform the whole dataset from the CoNLL format into the scikit-learn vectorized encoding using our feature extraction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, vec = vectorize(train_sentences, val_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression algorithm uses a hyper-parameter called C.  We tune the value of this parameter by testing different values on a subset of the training data and observing the impact of the C parameter on selected metrics (accuracy and F1).\n",
    "\n",
    "Because we will use cross-validation, we can use the full train set (train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vstack((X_train, X_val))\n",
    "y_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(train, test, y_train, y_test, scores, estimator, parameters, cv):\n",
    "    print(\"# Estimator:\",estimator)\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)        \n",
    "        clf = GridSearchCV(estimator, parameters, cv=cv, scoring='%s' % score)\n",
    "        clf.fit(train, y_train)\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        print()\n",
    "        print(\"Detailed classification report:\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        y_pred = clf.predict(test)\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        print('Accuracy: {0:0.4f}   f1-score: {1:0.4f} \\n'.format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregr = LogisticRegression(solver='liblinear', random_state=13)\n",
    "# Cross validation strategy\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# Scores could also be ['precision', 'recall', ....]\n",
    "scores = ['accuracy', 'f1_macro']\n",
    "\n",
    "params = [{'C': [0.1, 1, 2, 3, 4, 5, 10, 20, 50]}]\n",
    "#logregr = hyper_tuning(X_train, X_test, y_train, y_test, scores, logregr, params, skf)\n",
    "#You may want to comment previous line and comment-out next lines to see how hyper-tuning works and dont wait too much time...\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "logregr = hyper_tuning(X_train[:500], X_test[:50], y_train[:500], y_test[:50], scores, logregr, params, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now training using the best hyper-parameter selected above.  This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ini = datetime.datetime.now()\n",
    "print('Training...')\n",
    "clf = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(clf.score(X_test, y_test)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Types of Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "k=0\n",
    "i2w = id2word(test_sentences)\n",
    "error_counter = collections.Counter()\n",
    "for i in range(X_test.shape[0]):\n",
    "    correct_tag_id = y_test\n",
    "    if y_pred[i]!=y_test[i]:\n",
    "        k += 1\n",
    "        word = i2w[i]\n",
    "        error_counter[word] += 1\n",
    "print('Accuracy: {0:.4f}'.format((len(i2w)-k)/len(i2w)))\n",
    "print('Total errors/Total words: {}/{}\\n'.format(k,len(i2w)))\n",
    "print('Most common errors:',error_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates the \"training dynamics\" of the LR model: how fast does it improve as it keeps training. Originally, the difference between the test dataset and the cross-validation (on part of the test data) is large; as training proceeds, the gap reduces. This diagram is important to verify we do not have a case of over-fitting - where the model does \"very well\" on training data and does not improve on test data.  \n",
    "\n",
    "This computation takes a long time (as we keep training and evaluating multiple times to obtain the snapshots). It is not necessary to run the rest of the notebook so that you can safely skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_learning_curve(clf, 'Logistic Regression', X_train, y_train, ylim=(0.7, 1.01), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Logistic Regression'\n",
    "plot_confusion_matrix(f1, cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag an Unknown Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try our trained tagger on new sentences.  To tag a sentence given as a string, we must apply the following steps:\n",
    "* Tokenize the string into a list of tokens\n",
    "* Turn each token into a features dictionary (using the features used by our model)\n",
    "* Turn the list of feature dictionaries into vectors (using scikit-learn vectorization method)\n",
    "* Pass the resulting matrix (one row vector for each token) to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download the nltk model for sentence tokenizer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize('Word embeddings provide a dense representation of words and their relative meanings.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = transform_test_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vec.transform([x[0] for x in X_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorized sentence is a sparse matrix with one row for each token and columns for the vectorized features\n",
    "# For example, if the vocabulary has 1000 unique words, the vectorized sentence will have 1000 columns for each word feature.\n",
    "# This is a very sparse matrix, where most of the values are zero.\n",
    "X_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.str_ is a subclass of str that is used to represent string arrays in NumPy.\n",
    "print('Here is what our LR tagger predicts for the test sentence:\\n',list(zip(tokens, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this process into a prediction function from a sentence encoded as a single string to a list of pairs (token, predicted_tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "   tokens = nltk.word_tokenize(sentence)\n",
    "   X_features = transform_test_sentence(tokens)\n",
    "   X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "   pred = clf.predict(X_vectorized)\n",
    "   return list(zip(tokens, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence(\"Let me join the chorus of annoyance over Google's new toolbar , which, as noted in the linked article, commits just about every sin an online marketer could commit, and makes up a few new ones besides.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Hard Sentences\n",
    "\n",
    "Hard sentences are sentences that contain multiple wrongly predicted tags given our classifier.\n",
    "\n",
    "Write code to collect hard sentences given a classifier clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_in_sentence_prediction(clf, tagged_sentence):\n",
    "    \"\"\"Given a tagged sentence from the dataset, return the number of errors and the predicted tags.\"\"\"\n",
    "    errors = 0\n",
    "    tokens = [word for word, _ in tagged_sentence] \n",
    "    true_tags = [true_tag for _, true_tag in tagged_sentence] \n",
    "\n",
    "    X_features = transform_test_sentence(tokens)\n",
    "    X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "    pred = clf.predict(X_vectorized)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if true_tags[i] != pred[i]:\n",
    "            errors += 1\n",
    "\n",
    "    return errors, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_sentences = []\n",
    "idx = 0\n",
    "for s in test_sentences:\n",
    "    errors, pred = errors_in_sentence_prediction(clf, s)\n",
    "    if errors > 0:\n",
    "        hard_sentences.append((s, errors, pred, idx))\n",
    "    idx += 1\n",
    "print(f'Number of sentences with errors: {len(hard_sentences)} out of {len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a histogram showing how the sentences in the test dataset are distributed in terms of prediction errors per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a histogram of the number of errors per sentence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([x[1] for x in hard_sentences], bins=16, edgecolor='black')\n",
    "plt.title('Number of errors per sentence')\n",
    "plt.xlabel('Number of errors')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction errors for sentences with more than 5 errors.\n",
    "for s in hard_sentences:\n",
    "    if s[1] > 5:\n",
    "        print(\" \".join(untag(s[0])))\n",
    "        print('Number of errors:', s[1])\n",
    "        for i in range(len(s[0])):\n",
    "            if s[0][i][1] != s[2][i]:\n",
    "                print(f'{s[0][i][0]:<20}  C: {s[0][i][1]:<12}  P: {s[2][i]:<12} **** Error')\n",
    "            else:\n",
    "                print(f'{s[0][i][0]:<23}  {s[0][i][1]:<12}')\n",
    "        print(40*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "1. Identify tokens that are misclassified more than 10 times in the test set. Print the sentences where the errors are predicted (about 100 sentences).\n",
    "2. Provide a possible reason why these errors are made by the tagger based on your understanding of the knowledge needed to correctly tag these tokens.\n",
    "3. Based on this error analysis, invent five sentences that are badly tagged. Explain what is your method to create these hard examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = collections.defaultdict(list)\n",
    "sen_ids = [] \n",
    "for i, s in enumerate(hard_sentences):\n",
    "    words = [word for word, _ in hard_sentences[i][0]]\n",
    "    full_s = ' '.join(words)\n",
    "    sen_ids.append(full_s)\n",
    "    for j in range(len(s[0])):\n",
    "        if s[0][j][1] != s[2][j]:\n",
    "            tokens[s[0][j][0]].append(i)\n",
    "\n",
    "sentences = set()\n",
    "print(\"Tokens with more than 10 errors:\")\n",
    "for token, lst in tokens.items():\n",
    "    if len(lst) > 10:\n",
    "        print(token)\n",
    "        for index in lst:\n",
    "            sentences.add(sen_ids[index])\n",
    "\n",
    "print(\"Total amount of sentences:\", len(sentences))\n",
    "\n",
    "print(\"Sentences where the errors are predicted:\\n\")\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# \n",
    "model = 'grok-3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Universal Dependencies POS Tagset (17 core tags) as an enum ---\n",
    "class UDPosTag(str, Enum):\n",
    "    ADJ = \"ADJ\"     # adjective\n",
    "    ADP = \"ADP\"     # adposition\n",
    "    ADV = \"ADV\"     # adverb\n",
    "    AUX = \"AUX\"     # auxiliary verb\n",
    "    CCONJ = \"CCONJ\" # coordinating conjunction\n",
    "    DET = \"DET\"     # determiner\n",
    "    INTJ = \"INTJ\"   # interjection\n",
    "    NOUN = \"NOUN\"   # noun\n",
    "    NUM = \"NUM\"     # numeral\n",
    "    PART = \"PART\"   # particle\n",
    "    PRON = \"PRON\"   # pronoun\n",
    "    PROPN = \"PROPN\" # proper noun\n",
    "    PUNCT = \"PUNCT\" # punctuation\n",
    "    SCONJ = \"SCONJ\" # subordinating conjunction\n",
    "    SYM = \"SYM\"     # symbol\n",
    "    VERB = \"VERB\"   # verb\n",
    "    X = \"X\"         # other / unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define more Pydantic models for structured output\n",
    "class TokenPOS(BaseModel):\n",
    "    text: str = Field(description=\"The token text\")\n",
    "    pos_tag: UDPosTag = Field(description=\"The Universal Dependencies POS tag\")\n",
    "\n",
    "class SentencePOS(BaseModel):\n",
    "    tokens: List[TokenPOS] = Field(description=\"List of tokens with their POS tags\")\n",
    "\n",
    "class TaggedSentences(BaseModel):\n",
    "    \"\"\"Represents a list of sentences with their tagged tokens.\"\"\"\n",
    "    sentences: List[SentencePOS] = Field(description=\"A list of sentences, each containing tagged tokens.\")\n",
    "\n",
    "# --- Configure the Grok API ---\n",
    "# Get a key https://console.x.ai/team \n",
    "# Use os.environ.get for production environments.\n",
    "# For Colab/AI Studio, you might use userdata.get\n",
    "# Example:\n",
    "# from google.colab import userdata\n",
    "# GROK_API_KEY = userdata.get('GROK_API_KEY')\n",
    "# genai.configure(api_key=GROK_API_KEY)\n",
    "\n",
    "# Make sure to replace \"YOUR_API_KEY\" with your actual key if running locally\n",
    "# and not using environment variables or userdata.\n",
    "try:\n",
    "    # Attempt to get API key from environment variable\n",
    "    def load_env_from_ini(filename):\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "    # Load the API key\n",
    "    load_env_from_ini(\"grok_key.ini\")\n",
    "    api_key = os.environ.get(\"GROK_API_KEY\")\n",
    "    if not api_key:\n",
    "        # Fallback or specific instruction for local setup\n",
    "        # Replace with your actual key if needed, but environment variables are safer\n",
    "        api_key = \"YOUR_API_KEY\"\n",
    "        if api_key == \"YOUR_API_KEY\":\n",
    "           print(\"⚠️ Warning: API key not found in environment variables. Using placeholder.\")\n",
    "           print(\"   Please set the GROK_API_KEY environment variable or replace 'YOUR_API_KEY' in the code.\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://api.x.ai/v1\",\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring API: {e}\")\n",
    "    print(\"Please ensure you have a valid API key set.\")\n",
    "    # Depending on the environment, you might want to exit here\n",
    "    # import sys\n",
    "    # sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Add this import if not already present\n",
    "\n",
    "def tag_sentences_ud(sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on the input list of sentences using the Grok API and\n",
    "    returns the result structured according to the TaggedSentences Pydantic model.\n",
    "    \n",
    "    Args:\n",
    "        sentences_json: JSON string containing one or more sentences to tag\n",
    "        \n",
    "    Returns:\n",
    "        A TaggedSentences object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Construct the prompt with JSON input format\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions that outputs precise structured JSON.\n",
    "    \n",
    "    I will provide you with a JSON array of sentences to tag. Process each sentence separately.\n",
    "    \n",
    "    Tag each token with Universal Dependencies (UD) POS tags:\n",
    "    ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner, \n",
    "    INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun, \n",
    "    PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other\n",
    "    \n",
    "    Rules:\n",
    "    - Split on whitespace and punctuation (except in URLs, numbers, abbreviations)\n",
    "    - Split contractions: \"don't\" → [\"Do\", \"n't\"], \"it's\" → [\"It\", \"'s\"]\n",
    "    - Separate possessives: \"Elena's\" → [\"Elena\", \"'s\"]\n",
    "    - Split hyphenated compounds: \"search-engine\" → [\"search\", \"-\", \"engine\"]\n",
    "    - Keep punctuation as separate tokens\n",
    "    - Preserve numbers with internal periods/commas (e.g., 3.14, 1,000)\n",
    "    - Do not merge words except for contractions/clitics\n",
    "    - if there is a sentence equal to \"...\" tag it as \"PUNCT\", don't ignore it.\n",
    "    - make sure you don't skip any tokens or strings. tag all tokens.\n",
    "    \n",
    "    Input JSON array of sentences:\n",
    "    {sentences_json}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag(sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    return tag_sentences_ud(sentences_json)\n",
    "\n",
    "def batch_tag_sentences_ud(sentences: List[str], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"Process sentences in parallel with rate limiting using JSON formatting\"\"\"\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        # Convert batch to JSON string instead of using <sentence> tags\n",
    "        batch_json = json.dumps(batch)\n",
    "        batches.append(batch_json)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches with parallel workers...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag, batch_json): i \n",
    "                          for i, batch_json in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"✓ Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hard_sentences = [s for s in hard_sentences if 1 <= s[1] <= 3]\n",
    "sentences = [\" \".join(untag(pairs)) for pairs, _, _ , _ in lr_hard_sentences]\n",
    "results_llm = batch_tag_sentences_ud(sentences, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_hard_sentences = []\n",
    "\n",
    "\n",
    "def evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5):\n",
    "    \"\"\"\n",
    "    Evaluates the LLM tagger results against ground truth, handling potential misalignments.\n",
    "    \n",
    "    Args:\n",
    "        results_llm: List of TaggedSentences results from the LLM\n",
    "        lr_hard_sentences: List of (sentence, errors, lr_tags, idx) tuples with ground truth\n",
    "        batch_size: The batch size used when processing sentences\n",
    "    \n",
    "    Returns:\n",
    "        Metrics for the LLM tagger performance\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "    \n",
    "    # Filter out None results\n",
    "    results_llm = [r for r in results_llm if r is not None]\n",
    "    \n",
    "    # Create batch-aware mapping of results\n",
    "    batched_sentences = [lr_hard_sentences[i:i+batch_size] for i in range(0, len(lr_hard_sentences), batch_size)]\n",
    "    \n",
    "    # Initialize empty lists for predictions and words\n",
    "    llm_pred = []\n",
    "    llm_words = []\n",
    "    sentence_mapping = {}  # Maps original index to result index\n",
    "    \n",
    "    print(f\"Type of results_llm[0]: {type(results_llm[0]) if results_llm else 'No results'}\")\n",
    "    \n",
    "    # Process each batch separately\n",
    "    for batch_idx, batch in enumerate(batched_sentences):\n",
    "        if batch_idx >= len(results_llm):\n",
    "            print(f\"Missing results for batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        batch_result = results_llm[batch_idx]\n",
    "        \n",
    "        # Check if it has sentences attribute\n",
    "        if hasattr(batch_result, 'sentences'):\n",
    "            result_sentences = batch_result.sentences\n",
    "        elif hasattr(batch_result, 'tokens'):\n",
    "            # Handle single-sentence result case\n",
    "            result_sentences = [batch_result]\n",
    "        else:\n",
    "            print(f\"Unknown structure for batch {batch_idx}: {dir(batch_result)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if we got the expected number of sentences\n",
    "        if len(result_sentences) != len(batch):\n",
    "            print(f\"Warning: Batch {batch_idx} returned {len(result_sentences)} sentences, expected {len(batch)}\")\n",
    "        \n",
    "        # Map each input sentence to its result (or None if missing)\n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(batch):\n",
    "            orig_idx = batch_idx * batch_size + i  # Original index in lr_hard_sentences\n",
    "            \n",
    "            if i < len(result_sentences):\n",
    "                # We have a result for this sentence\n",
    "                pred = [token.pos_tag for token in result_sentences[i].tokens]\n",
    "                words = [token.text for token in result_sentences[i].tokens]\n",
    "                llm_pred.append(pred)\n",
    "                llm_words.append(words)\n",
    "                sentence_mapping[orig_idx] = len(llm_pred) - 1\n",
    "            else:\n",
    "                # No result for this sentence\n",
    "                print(f\"No prediction available for sentence {orig_idx}\")\n",
    "                # Don't add to llm_pred/llm_words, but record this in mapping\n",
    "                sentence_mapping[orig_idx] = None\n",
    "    \n",
    "    # Only proceed with metrics if we have predictions\n",
    "    if len(llm_pred) > 0:\n",
    "        # Initialize counters and lists\n",
    "        global llm_hard_sentences\n",
    "        llm_hard_sentences = []\n",
    "        fixed_by_llm = 0\n",
    "        new_errors_by_llm = 0\n",
    "        llm_error_data = []\n",
    "        mismatches = 0\n",
    "        \n",
    "        # For metrics calculation\n",
    "        all_true_tags = []\n",
    "        all_pred_tags = []\n",
    "        all_words = []\n",
    "        \n",
    "        for i, (s, errors, lr_tags, idx) in enumerate(lr_hard_sentences):\n",
    "            # Use the mapping to get the correct prediction index\n",
    "            if i not in sentence_mapping or sentence_mapping[i] is None:\n",
    "                print(f\"Skipping sentence {i} - no prediction available\")\n",
    "                continue\n",
    "                \n",
    "            pred_idx = sentence_mapping[i]\n",
    "            words = [word for word, _ in s]\n",
    "            true_tags = [true_tag for _, true_tag in s]\n",
    "            pred_tags = llm_pred[pred_idx]\n",
    "            \n",
    "            # Sanity check: length of tokens should match\n",
    "            if len(true_tags) != len(pred_tags) or len(pred_tags) != len(words):\n",
    "                print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, pred={len(pred_tags)}\")\n",
    "                # print(llm_words[pred_idx])\n",
    "                # print(words)\n",
    "                mismatches += 1\n",
    "                continue\n",
    "            \n",
    "            # Add to the collections for metrics calculations\n",
    "            all_true_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in true_tags])\n",
    "            all_pred_tags.extend([tag.value if hasattr(tag, 'value') else str(tag) for tag in pred_tags])\n",
    "            all_words.extend(words)\n",
    "\n",
    "\n",
    "            llm_errors = 0\n",
    "            for j in range(len(words)):\n",
    "                if true_tags[j] != pred_tags[j]:\n",
    "                    llm_errors += 1\n",
    "                    llm_error_data.append((words[j], true_tags[j], pred_tags[j]))\n",
    "                    \n",
    "                lr_wrong = lr_tags[j] != true_tags[j]\n",
    "                llm_wrong = pred_tags[j] != true_tags[j]\n",
    "                \n",
    "                if lr_wrong and not llm_wrong:\n",
    "                    fixed_by_llm += 1\n",
    "                if not lr_wrong and llm_wrong:\n",
    "                    new_errors_by_llm += 1\n",
    "                    \n",
    "            if llm_errors > 0:\n",
    "                llm_hard_sentences.append((s, llm_errors))\n",
    "        \n",
    "        # Print comparison metrics\n",
    "        print(f'Number of sentences with errors (llm): {len(llm_hard_sentences)} out of {len(lr_hard_sentences)}')\n",
    "        print(f'Number of sentences with errors (lr): {len(lr_hard_sentences)}')\n",
    "        print(f\"✅ Errors fixed by LLM: {fixed_by_llm}\")\n",
    "        print(f\"⚠️ New errors made by LLM: {new_errors_by_llm}\")\n",
    "        print(f\"❗️ Mismatches in sentence length: {mismatches}\")\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        if len(all_true_tags) > 0:\n",
    "            print(\"\\n--- LLM Tagger Token-Level Metrics ---\")\n",
    "            llm_accuracy = accuracy_score(all_true_tags, all_pred_tags)\n",
    "            llm_f1_macro = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            print(f\"Accuracy: {llm_accuracy:.4f}\")\n",
    "            print(f\"F1-macro score: {llm_f1_macro:.4f}\")\n",
    "            \n",
    "            # Generate classification report\n",
    "            print(\"\\nClassification Report for LLM Tagger:\")\n",
    "            print(classification_report(all_true_tags, all_pred_tags, digits=4))\n",
    "            \n",
    "            # Generate confusion matrix\n",
    "            classes = sorted(list(set(all_true_tags)))\n",
    "            cnf_matrix = confusion_matrix(all_true_tags, all_pred_tags)\n",
    "            f1 = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "            plot_confusion_matrix(f1, cnf_matrix, target_names=classes, \n",
    "                                title='Confusion matrix for LLM Tagger', normalize=False)\n",
    "            \n",
    "            # Count and display most common errors\n",
    "            error_counter = collections.Counter()\n",
    "            total_errors = 0\n",
    "            for i in range(len(all_true_tags)):\n",
    "                if all_true_tags[i] != all_pred_tags[i]:\n",
    "                    total_errors += 1\n",
    "                    word = all_words[i]\n",
    "                    error_counter[word] += 1\n",
    "            \n",
    "            print(\"\\nFrequent Types of Mistakes:\")\n",
    "            print(f'Accuracy: {(len(all_true_tags)-total_errors)/len(all_true_tags):.4f}')\n",
    "            print(f'Total errors/Total words: {total_errors}/{len(all_true_tags)}\\n')\n",
    "            print('Most common errors:', error_counter.most_common(20))\n",
    "            \n",
    "            return {\n",
    "                'accuracy': llm_accuracy,\n",
    "                'f1_macro': llm_f1_macro,\n",
    "                'fixed_by_llm': fixed_by_llm,\n",
    "                'new_errors_by_llm': new_errors_by_llm,\n",
    "                'mismatches': mismatches,\n",
    "                'error_data': llm_error_data,\n",
    "                'hard_sentences': llm_hard_sentences\n",
    "            }\n",
    "        else:\n",
    "            print(\"No data available to calculate metrics\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No LLM predictions available to calculate metrics\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import concurrent\n",
    "from ratelimit import limits, sleep_and_retry \n",
    "\n",
    "def tag_pretokenized_sentences(tokenized_sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on pre-tokenized sentences using the Grok API.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences_json: JSON string containing a list of tokenized sentences, \n",
    "                                  where each sentence is a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        A TaggedSentences object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Construct the prompt with pre-tokenized input format\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions.\n",
    "    \n",
    "    I will provide you with a JSON array where each element is a pre-tokenized sentence (a list of tokens).\n",
    "    Your task is ONLY to assign the correct Universal Dependencies POS tag to each token.\n",
    "    DO NOT modify the tokenization in any way - use exactly the tokens provided.\n",
    "    \n",
    "    Tag each token with one of these Universal Dependencies POS tags:\n",
    "    ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner,\n",
    "    INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun,\n",
    "    PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other\n",
    "    \n",
    "    Important guidelines:\n",
    "    - DO NOT change, merge, or split any tokens - use exactly the tokens provided\n",
    "    - If a token is \"...\" tag it as PUNCT\n",
    "    - Tag all tokens - do not skip any\n",
    "    - Contractions like \"n't\" should be tagged as PART\n",
    "    - Possessive markers like \"'s\" should be tagged as PART\n",
    "    - Proper nouns (names of specific entities) should be tagged as PROPN\n",
    "    - Auxiliary verbs (be, have, do, will, etc.) should be tagged as AUX\n",
    "    \n",
    "    Input JSON array of pre-tokenized sentences:\n",
    "    {tokenized_sentences_json}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    \n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag_pretokenized(tokenized_sentences_json: str) -> Optional[TaggedSentences]:\n",
    "    return tag_pretokenized_sentences(tokenized_sentences_json)\n",
    "\n",
    "def batch_tag_pretokenized_sentences(tokenized_sentences: List[List[str]], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Process pre-tokenized sentences in batches using JSON formatting\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of tokenized sentences, where each sentence is a list of tokens\n",
    "        batch_size: Number of sentences to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        List of TaggedSentences results\n",
    "    \"\"\"\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(tokenized_sentences), batch_size):\n",
    "        batch = tokenized_sentences[i:i+batch_size]\n",
    "        # Convert batch to JSON string\n",
    "        batch_json = json.dumps(batch)\n",
    "        batches.append(batch_json)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches of pre-tokenized sentences...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag_pretokenized, batch_json): i \n",
    "                          for i, batch_json in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"✓ Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# lr_hard_sentences = [s for s in hard_sentences if 1 <= s[1] <= 3]\n",
    "\n",
    "# # Extract pre-tokenized sentences (using the untag function you already have)\n",
    "# tokenized_sentences = [untag(pairs) for pairs, _, _, _ in lr_hard_sentences]\n",
    "\n",
    "# # Process the pre-tokenized sentences\n",
    "# results_llm = batch_tag_pretokenized_sentences(tokenized_sentences, batch_size=5)\n",
    "\n",
    "# Evaluate with your evaluation function\n",
    "metrics = evaluate_llm_tagger(results_llm, lr_hard_sentences, batch_size=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "\n",
    "# Plot the histogram for LR\n",
    "ax[0].hist([x[1] for x in lr_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[0].set_title(\"LR Tagger Error Histogram\")\n",
    "ax[0].set_xlabel(\"Number of Errors\")\n",
    "ax[0].set_ylabel(\"Number of Sentences\")\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Plot the histogram for LLM\n",
    "ax[1].hist([errors for s, errors in llm_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[1].set_title(\"LLM Tagger Error Histogram\")\n",
    "ax[1].set_xlabel(\"Number of Errors\")\n",
    "ax[1].set_ylabel(\"Number of Sentences\")\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ErrorExplanation(BaseModel):\n",
    "    word: str\n",
    "    correct_tag: str\n",
    "    predicted_tag: str\n",
    "    explanation: str\n",
    "    category: str   \n",
    "\n",
    "def explain_tagging_errors(\n",
    "    errors: List[Tuple[str, str, \"UDPosTag\"]],\n",
    "    sentence_context: str = \"The error word appeared in a sentence. You may assume typical usage.\",\n",
    "    max_errors: int = 20,\n",
    "    delay: float = 1.0\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses Grok to explain POS tagging errors.\n",
    "\n",
    "    Args:\n",
    "        errors: A list of (word, predicted_tag, correct_tag) tuples.\n",
    "        sentence_context: Optional sentence to help Grok understand usage.\n",
    "        max_errors: Maximum number of errors to process.\n",
    "        delay: Seconds to wait between requests.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries with keys: word, correct_tag, predicted_tag, explanation, category.\n",
    "    \"\"\"\n",
    "    explanations = []\n",
    "\n",
    "    for word, predicted_tag, correct_tag in errors[:max_errors]:\n",
    "        prompt = f\"\"\"\n",
    "    You are a linguistics expert.\n",
    "\n",
    "    A POS tagging model made the following error:\n",
    "    - Word: {word}\n",
    "    - Correct tag: {correct_tag.value}\n",
    "    - Predicted tag: {predicted_tag}\n",
    "    - Sentence context: {sentence_context}\n",
    "\n",
    "    Explain in 1–3 sentences why this tagging error likely occurred, using a clear linguistic explanation (e.g., idiom, function word, capitalization confusion, named entity confusion, etc.).\n",
    "\n",
    "    Then provide a **concise category** that captures the main cause of the error.\n",
    "    ⚠️ The category must be **one of the following** (choose the best match):\n",
    "\n",
    "    - Function word misclassification\n",
    "    - Capitalization\n",
    "    - Named entity issue\n",
    "    - Punctuation influence\n",
    "    - Contextual ambiguity\n",
    "    - Preposition/Adverb confusion\n",
    "    - Model bias\n",
    "    - Idiomatic expression\n",
    "    - Word frequency bias\n",
    "    - Tokenization mismatch\n",
    "\n",
    "    Do **not** create new category names. Only pick from the list above.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - word\n",
    "    - correct_tag\n",
    "    - predicted_tag\n",
    "    - explanation\n",
    "    - category\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format=ErrorExplanation,\n",
    "            )\n",
    "            parsed: ErrorExplanation = completion.choices[0].message.parsed\n",
    "            explanations.append(parsed.dict())  # Convert to plain dict\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining word '{word}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def format_explanations_as_json_block(explanations: list) -> str:\n",
    "    formatted = []\n",
    "\n",
    "    for e in explanations:\n",
    "        word = e[\"word\"].upper()\n",
    "        correct = e[\"correct_tag\"]\n",
    "        predicted = e[\"predicted_tag\"]\n",
    "\n",
    "        # Try to extract short category + tag pair\n",
    "        if \"/\" in e[\"category\"]:\n",
    "            parts = e[\"category\"].split(\"/\")\n",
    "            if len(parts) == 2:\n",
    "                tag1, tag2 = parts[0].strip(), parts[1].split()[0].strip()  # split off 'ambiguity'\n",
    "                category = f\"Ambiguity ({tag1}/{tag2})\"\n",
    "            elif len(parts) == 3:  # e.g. \"ADP/ADV ambiguity\"\n",
    "                tag1, tag2 = parts[0].strip(), parts[1].strip()\n",
    "                category = f\"Ambiguity ({tag1}/{tag2})\"\n",
    "            else:\n",
    "                category = e[\"category\"].capitalize()\n",
    "        else:\n",
    "            category = e[\"category\"].capitalize()\n",
    "\n",
    "        formatted.append({\n",
    "            \"word\": word,\n",
    "            \"correct_tag\": correct,\n",
    "            \"predicted_tag\": predicted,\n",
    "            \"explanation\": e[\"explanation\"].strip(),\n",
    "            \"category\": category\n",
    "        })\n",
    "\n",
    "    return \"JSON\\n\" + json.dumps(formatted, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = explain_tagging_errors(llm_error_data[0:20])\n",
    "print(format_explanations_as_json_block(explanations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "category_counter = Counter([e[\"category\"] for e in explanations])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort categories by frequency\n",
    "sorted_items = sorted(category_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "categories, counts = zip(*sorted_items)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(categories, counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Number of Errors\")\n",
    "plt.title(\"Error Categories in LLM Tagging\")\n",
    "plt.gca().invert_yaxis()  # Largest on top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for cat, count in category_counter.most_common():\n",
    "    print(f\"{cat}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import List\n",
    "from pydantic import BaseModel, RootModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Define schemas\n",
    "class SyntheticSentence(BaseModel):\n",
    "    sentence: List[str]\n",
    "    tags: List[str]\n",
    "    categories: List[str]  # ✅ added\n",
    "\n",
    "class SyntheticBatch(RootModel[List[SyntheticSentence]]):\n",
    "    pass\n",
    "\n",
    "# ✅ Predefined error categories\n",
    "error_categories = [\n",
    "    \"Function word misclassification\",\n",
    "    \"Capitalization\",\n",
    "    \"Named entity issue\",\n",
    "    \"Punctuation influence\",\n",
    "    \"Contextual ambiguity\",\n",
    "    \"Preposition/Adverb confusion\",\n",
    "    \"Model bias\",\n",
    "    \"Idiomatic expression\",\n",
    "    \"Word frequency bias\",\n",
    "    \"Tokenization mismatch\"\n",
    "]\n",
    "\n",
    "# ✅ Build prompt\n",
    "def build_synthetic_prompt(categories: List[str]) -> str:\n",
    "    joined = \", \".join(categories)\n",
    "    return f\"\"\"\n",
    "Generate 2 English sentences that demonstrate POS tagging challenges involving: {joined}.\n",
    "\n",
    "For each sentence, return:\n",
    "- A list of tokens\n",
    "- Their corresponding UD POS tags\n",
    "\n",
    "Return a valid JSON list like:\n",
    "[\n",
    "  {{\n",
    "    \"sentence\": [...],\n",
    "    \"tags\": [...]\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Only return JSON. Do not explain.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Request a single batch with retry and category attachment\n",
    "def generate_batch(categories: List[str], max_retries=3) -> List[SyntheticSentence]:\n",
    "    prompt = build_synthetic_prompt(categories)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                response_format=SyntheticBatch,\n",
    "            )\n",
    "            results = completion.choices[0].message.parsed.root\n",
    "            # ✅ attach categories to each result\n",
    "            for r in results:\n",
    "                r.categories = categories\n",
    "            return [r for r in results if len(r.sentence) == len(r.tags)]\n",
    "        except Exception as e:\n",
    "            wait = 2 + random.uniform(0, 2)\n",
    "            print(f\"[Retry {attempt+1}] Error: {e} — waiting {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    print(\"❌ Failed after retries.\")\n",
    "    return []\n",
    "\n",
    "# ✅ Run in parallel\n",
    "def run_parallel_generation(n_batches: int = 200, n_threads: int = 5) -> List[SyntheticSentence]:\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(generate_batch, random.sample(error_categories, k=3))\n",
    "            for _ in range(n_batches)\n",
    "        ]\n",
    "        for future in tqdm(as_completed(futures), total=n_batches):\n",
    "            batch = future.result()\n",
    "            all_results.extend(batch)\n",
    "    return all_results\n",
    "\n",
    "# ✅ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    n_batches = 100   # 100 batches × 2 sentences = ~200\n",
    "    n_threads = 5     # Adjust as needed\n",
    "\n",
    "    results = run_parallel_generation(n_batches=n_batches, n_threads=n_threads)\n",
    "\n",
    "    print(f\"\\n✅ Done! Generated {len(results)} synthetic sentences.\")\n",
    "\n",
    "    # ✅ Print some examples with categories\n",
    "    for i, r in enumerate(results[:10]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(\"Sentence: \", \" \".join(r.sentence))\n",
    "        print(\"Tags:     \", r.tags)\n",
    "        print(\"Categories:\", r.categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_synthetic_to_tagged_sentences(synthetic: List[SyntheticSentence]):\n",
    "    return [list(zip(s.sentence, s.tags)) for s in synthetic]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    n_batches = 100\n",
    "    n_threads = 5\n",
    "    # print(\"Generating synthetic data...\")\n",
    "    # results = run_parallel_generation(n_batches=n_batches, n_threads=n_threads)\n",
    "\n",
    "    # print(f\"\\n Done! Generated {len(results)} synthetic sentences.\")\n",
    "    # for i, r in enumerate(results[:5]):\n",
    "    #     print(f\"\\n--- Example {i+1} ---\")\n",
    "    #     print(\"Sentence: \", \" \".join(r.sentence))\n",
    "    #     print(\"Tags:     \", r.tags)\n",
    "    #     print(\"Categories:\", r.categories)\n",
    "\n",
    "    # Convert to training format and combine\n",
    "    synthetic_tagged = convert_synthetic_to_tagged_sentences(results)\n",
    "    train_augmented = train_sentences + synthetic_tagged * 3 \n",
    "    random.shuffle(train_augmented)\n",
    "\n",
    "    # Vectorize with your existing function\n",
    "    print(\"Vectorizing data...\")\n",
    "    X_train_synth, y_train_synth, X_val_synth, y_val_synth, X_test_synth, y_test_synth, vectorizer_synth = vectorize(train_augmented, val_sentences, test_sentences)\n",
    "\n",
    "    # Train the LR tagger\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    t_ini = datetime.datetime.now()\n",
    "    clf_synth = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "    clf_synth.fit(X_train_synth, y_train_synth)\n",
    "    t_fin = datetime.datetime.now()\n",
    "    print(f\"Training completed in {(t_fin - t_ini).total_seconds():.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_orig = clf.predict(X_test)\n",
    "y_pred_synth = clf_synth.predict(X_test_synth)\n",
    "\n",
    "# Track positions where the original was wrong but the synthetic fixed it, and vice versa\n",
    "fixed_by_synth = []\n",
    "regressed_by_synth = []\n",
    "unchanged_errors = []\n",
    "\n",
    "for i, (true, pred_orig, pred_synth) in enumerate(zip(y_test_synth, y_pred_orig, y_pred_synth)):\n",
    "    if pred_orig != true and pred_synth == true:\n",
    "        fixed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig == true and pred_synth != true:\n",
    "        regressed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig != true and pred_synth != true:\n",
    "        unchanged_errors.append((i, true, pred_orig, pred_synth))\n",
    "\n",
    "print(\"Contrastive Error Analysis\")\n",
    "print(f\"Fixed errors by synthetic model: {len(fixed_by_synth)}\")\n",
    "print(f\"New errors introduced by synthetic model: {len(regressed_by_synth)}\")\n",
    "print(f\"Unchanged errors (both models wrong): {len(unchanged_errors)}\")\n",
    "\n",
    "print(\"\\n--- Original Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_orig, digits=3))\n",
    "\n",
    "print(\"\\n--- Synthetic-Augmented Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_synth, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_failed_with_original = []\n",
    "total_errors = 0\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(test_sentences)), 30)\n",
    "\n",
    "for i in sample_indices:\n",
    "    sentence = test_sentences[i]\n",
    "    true_tags = untag_pos(sentence)\n",
    "    errors_org = 0\n",
    "    errors_tok = 0  \n",
    "\n",
    "    tokenized_res = tag_sentences_ud(\" \".join(untag(sentence)))\n",
    "    original_res = tag_sentences_ud(test_original[i])\n",
    "\n",
    "    tokenized_tags = [[token.pos_tag.value for token in sentence.tokens] for sentence in tokenized_res.sentences]\n",
    "    tokenized_words = [[token.text for token in sentence.tokens] for sentence in tokenized_res.sentences]\n",
    "    original_tags = [[token.pos_tag.value for token in sentence.tokens] for sentence in original_res.sentences]\n",
    "    original_words = [[token.text for token in sentence.tokens] for sentence in original_res.sentences]\n",
    "    print(f\"tokenized: {tokenized_words}\")\n",
    "    print(f\"original: {original_words}\")\n",
    "    print(f\"true: {untag(sentence)}\")\n",
    "    \n",
    "    if not true_tags or not original_tags or not tokenized_tags or len(true_tags) != len(original_tags[0]) or len(true_tags) != len(tokenized_tags[0]) or len(tokenized_tags[0]) != len(original_tags[0]):\n",
    "        print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, original={len(original_tags[0])}, tokenized={len(tokenized_tags[0])}\")\n",
    "        if len(true_tags) != len(original_tags[0]) and len(true_tags) == len(tokenized_tags[0]):\n",
    "            sentences_failed_with_original.append(sentence)\n",
    "        continue\n",
    "    \n",
    "    for j in range(len(true_tags)):\n",
    "        if true_tags[j] != original_tags[0][j]:\n",
    "            errors_org += 1\n",
    "        if true_tags[j] != tokenized_tags[0][j]:\n",
    "            errors_tok += 1\n",
    "\n",
    "    if errors_org > 0 and errors_tok == 0:\n",
    "        sentences_failed_with_original.append(sentence)  \n",
    "    else:\n",
    "        print(f\"original: {errors_org} tokenized: {errors_tok}\")  \n",
    "    if errors_org > 0:\n",
    "        total_errors += 1\n",
    "    \n",
    "\n",
    "print(f\"Sentences that failed with the original text but not with the tokenized version: {len(sentences_failed_with_original)}\")\n",
    "print(f\"Performance on original text: {total_errors} errors out of {len(sample_indices)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# --- Step 1: Grok-based segmentation ---\n",
    "def segment_sentence_ud(text: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Segments a raw sentence into UD-style tokens using Grok.\n",
    "    Returns a list of tokens.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a linguist. Segment the following English sentence into word tokens \n",
    "according to the Universal Dependencies (UD) English guidelines used for CoNLL POS tagging.\n",
    "\n",
    "Do not include POS tags. Just return a JSON list of tokens.\n",
    "Split contractions like \"can't\" into [\"ca\", \"n't\"], punctuation as separate tokens, etc.\n",
    "\n",
    "Sentence:\n",
    "{text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"grok-3\",\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "            response_format=List[str],\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(\"✗ Error:\", e)\n",
    "        return None\n",
    "\n",
    "# --- Step 2: Evaluation metrics ---\n",
    "def compute_segmentation_metrics(gold_lists, pred_lists):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    exact_match = 0\n",
    "\n",
    "    for gold, pred in zip(gold_lists, pred_lists):\n",
    "        gold_set = set(gold)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        tp += len(gold_set & pred_set)\n",
    "        fp += len(pred_set - gold_set)\n",
    "        fn += len(gold_set - pred_set)\n",
    "\n",
    "        if gold == pred:\n",
    "            exact_match += 1\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    exact = exact_match / len(gold_lists)\n",
    "\n",
    "    print(\"📊 Segmentation Metrics:\")\n",
    "    print(f\"  Precision:     {precision:.3f}\")\n",
    "    print(f\"  Recall:        {recall:.3f}\")\n",
    "    print(f\"  F1 Score:      {f1:.3f}\")\n",
    "    print(f\"  Exact match:   {exact:.3f}\")\n",
    "\n",
    "    return precision, recall, f1, exact\n",
    "\n",
    "# --- Step 3: Evaluation loop ---\n",
    "def evaluate_segmenter(sentences_failed_with_original: List[List[tuple]]):\n",
    "    gold_tokens_all = []\n",
    "    pred_tokens_all = []\n",
    "\n",
    "    for tagged_sentence in sentences_failed_with_original:\n",
    "        raw_text = \" \".join(word for word, _ in tagged_sentence)\n",
    "        gold_tokens = [word for word, _ in tagged_sentence]\n",
    "        pred_tokens = segment_sentence_ud(raw_text)\n",
    "\n",
    "        if pred_tokens is None:\n",
    "            continue\n",
    "\n",
    "        gold_tokens_all.append(gold_tokens)\n",
    "        pred_tokens_all.append(pred_tokens)\n",
    "\n",
    "        # Optional: inspect\n",
    "        if gold_tokens != pred_tokens:\n",
    "            print(\"\\n⚠️ Mismatch example:\")\n",
    "            print(\"Raw:     \", raw_text)\n",
    "            print(\"Gold:    \", gold_tokens)\n",
    "            print(\"Predicted:\", pred_tokens)\n",
    "\n",
    "        time.sleep(0.2)  # be gentle with Grok\n",
    "\n",
    "    return compute_segmentation_metrics(gold_tokens_all, pred_tokens_all)\n",
    "\n",
    "evaluate_segmenter(sentences_failed_with_original)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
