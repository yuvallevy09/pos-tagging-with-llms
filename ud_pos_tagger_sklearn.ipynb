{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger Trained on the UD Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/soutsios/pos_tagger_mlp/blob/master/pos_tagger_mlp.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Functions\n",
    "\n",
    "These functions are useful to visualize the training dynamics of the learning algorithm and the confusion matrix of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll, nltk, datetime, warnings\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(f1,\n",
    "                          cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          i=1):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_ENGLISH_TRAIN = '../UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "UD_ENGLISH_DEV = '../UD_English-EWT/en_ewt-ud-dev.conllu'\n",
    "UD_ENGLISH_TEST = '../UD_English-EWT/en_ewt-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    original_sentences=[]\n",
    "    t=0\n",
    "    for sentence in data:\n",
    "        original_sentences.append(sentence.text)\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos:\n",
    "                t+=1\n",
    "                tagged_sentence.append((token.form if token.form else '*None*', token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences, original_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train, development and test set in the appropriate tagged format, tuple (word, pos-tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_original = read_conllu(UD_ENGLISH_TRAIN)\n",
    "val_sentences, val_original = read_conllu(UD_ENGLISH_DEV)\n",
    "test_sentences, test_original = read_conllu(UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def id2word(sentences):\n",
    "    wordlist = [item for sublist in text_sequence(sentences) for item in sublist]\n",
    "    id2word = {k:v for k,v in enumerate(wordlist)}\n",
    "    return id2word\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def untag_pos(tagged_sentence):\n",
    "    return [t for _, t in tagged_sentence]\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab =set()\n",
    "    for sentence in sentences:\n",
    "        for word in untag(sentence):\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important observation: how many terms are in validation set and not found in train set? (This estimates the Out-of-vocabulary rate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = build_vocab(train_sentences)\n",
    "list_2 = build_vocab(val_sentences)\n",
    "diff_list = [item for item in list_2 if item not in list_1]\n",
    "print('Number of terms not found in train set:',len(diff_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We use the \"classical\" machine learning approach: we will train a token classifier model. The classifier gets as input a feature vector describing each token in the sentence. We decide a priori which features are informative to make the tagging decision. In this case, we use a combination of \"word shape\" features which approximate morphological knowledge. We naturally also include lexical information (the token form itself), and some form of \"syntactic knowledge\" by adding reference to the previous and next word in each token feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_basic(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'nb_terms': len(sentence),        \n",
    "        'word': sentence[index],\n",
    "        'word_lower': sentence[index].lower(),\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'i-1_prefix-3': '' if index == 0 else sentence[index-1][:3],        \n",
    "        'i-1_suffix-3': '' if index == 0 else sentence[index-1][-3:],        \n",
    "        'i+1_prefix-3': '' if index == len(sentence) - 1 else sentence[index+1][:3],        \n",
    "        'i+1_suffix-3': '' if index == len(sentence) - 1 else sentence[index+1][-3:],        \n",
    "        'prev_word': '' if index == 0 else sentence[index - 1].lower(),\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1].lower(),\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scikit-learn model, we model a dataset as a pair of two data structures:\n",
    "* The list of feature dictionaries X (one feature dictionary for each token)\n",
    "* The list of predicted label y (one tag for each token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for doc_index, tagged in enumerate(tagged_sentences):\n",
    "        for index in range(len(tagged)):\n",
    "            X.append([features_basic(untag(tagged), index)])\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence represented as a list of tokens, return the list of feature dictionaries using our feature encoding method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_sentence(sentence):\n",
    "    X = []\n",
    "    for index in range(len(sentence)):\n",
    "            X.append([features_basic(sentence, index)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test untag()\n",
    "\n",
    "We use untag() to extract raw sentences from the annotated CoNLL dataset. This way we can reproduce a sentence without tags, submit it to the tagger and compare predictions to the gold tags that are provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untag(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply a generic machine learning algorithm (such as Logistic Regression), we need to encode the dataset into a vectorized format.\n",
    "\n",
    "We proceed in two steps: feature engineering and vectorization.\n",
    "\n",
    "For each token, we create a dictionary of features that depend on the sentence from which the token is extracted. \n",
    "These features include the word itself, the word before and the word after, letter suffixes and prefixes, etc.\n",
    "\n",
    "In the scikit-learn approach, before we can use a generic machine learning algorithm, we must then \"vectorize\" the feature dictionaries into vector encodings.\n",
    "For example, lexical features are encoded into one-hot vectors whose dimension is the size of the vocabulary.\n",
    "Note the difference between the method `fit_transform` of the vectorizer, which \"learns\" how to vectorize features, and `transform` which applies a learned vectorizer to feature dictionaries.  We use `fit_transform` on the training data, and `transform` on the other sections (validation and test).\n",
    "\n",
    "These vector representations are what is passed to the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, val, test):\n",
    "\n",
    "    print('Feature encoding method')\n",
    "    print('Vectorizing Dataset...')\n",
    "    print('Vectorizing train...')\n",
    "    X_train, y_train = transform_to_dataset(train)\n",
    "    v = DictVectorizer(sparse=True) \n",
    "    X_train = v.fit_transform([x[0] for x in X_train])\n",
    "    \n",
    "    print('Vectorizing validation...')\n",
    "    X_val, y_val = transform_to_dataset(val)\n",
    "    X_val = v.transform([x[0] for x in X_val])        \n",
    "    \n",
    "    print('Vectorizing test...')\n",
    "    X_test, y_test = transform_to_dataset(test)\n",
    "    X_test = v.transform([x[0] for x in X_test])\n",
    "    \n",
    "    print('Dataset vectorized.')\n",
    "    print('Train shape:', X_train.shape)\n",
    "    print('Validation shape:', X_val.shape)\n",
    "    print('Test shape:', X_test.shape)\n",
    "    \n",
    "    # Compress sparse matrices\n",
    "    X_train = X_train \n",
    "    X_val = X_val \n",
    "    X_test = X_test\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a \"real\" machine learning algorithm using scikit-learn, we will repeat the very simple statistical method we discussed in class.\n",
    "We train and evaluate the Baseline Unigram tagger to compare performance with the tagger we will train next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "unigram_tagger = nltk.UnigramTagger(train_sentences+val_sentences, backoff=default_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [item for sublist in tag_sequence(train_sentences+val_sentences) for item in sublist]\n",
    "y_test = [item for sublist in tag_sequence(test_sentences) for item in sublist]\n",
    "classes = sorted(list(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tag_list(apply_tagger(unigram_tagger, test_sentences))\n",
    "print(\"Accuracy: {0:.4f}\".format(unigram_tagger.accuracy(test_sentences)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, zero_division=1, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=1, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what errors the Baseline tagger makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tagger(tagged_sentence):\n",
    "    note = ''\n",
    "    for tup in list(zip(unigram_tagger.tag(untag(tagged_sentence)),untag_pos(tagged_sentence))):\n",
    "        if tup[0][1]!=tup[1]: note='<<--- Error!'\n",
    "        print(tup[0], tup[1], note)\n",
    "        note=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tagger(test_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes mistakes. Unsurprising given its simplistic approach and the small size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(f1_score(y_test, y_pred, average='macro'), cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move to a more serious machine learning model. We will train a Logistic Regression model using our feature extraction function based on our \"expertise\" in the domain.\n",
    "\n",
    "We first transform the whole dataset from the CoNLL format into the scikit-learn vectorized encoding using our feature extraction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, vec = vectorize(train_sentences, val_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression algorithm uses a hyper-parameter called C.  We tune the value of this parameter by testing different values on a subset of the training data and observing the impact of the C parameter on selected metrics (accuracy and F1).\n",
    "\n",
    "Because we will use cross-validation, we can use the full train set (train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vstack((X_train, X_val))\n",
    "y_train = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(train, test, y_train, y_test, scores, estimator, parameters, cv):\n",
    "    print(\"# Estimator:\",estimator)\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)        \n",
    "        clf = GridSearchCV(estimator, parameters, cv=cv, scoring='%s' % score)\n",
    "        clf.fit(train, y_train)\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        print()\n",
    "        print(\"Detailed classification report:\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        y_pred = clf.predict(test)\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        print('Accuracy: {0:0.4f}   f1-score: {1:0.4f} \\n'.format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregr = LogisticRegression(solver='liblinear', random_state=13)\n",
    "# Cross validation strategy\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# Scores could also be ['precision', 'recall', ....]\n",
    "scores = ['accuracy', 'f1_macro']\n",
    "\n",
    "params = [{'C': [0.1, 1, 2, 3, 4, 5, 10, 20, 50]}]\n",
    "#logregr = hyper_tuning(X_train, X_test, y_train, y_test, scores, logregr, params, skf)\n",
    "#You may want to comment previous line and comment-out next lines to see how hyper-tuning works and dont wait too much time...\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "logregr = hyper_tuning(X_train[:500], X_test[:50], y_train[:500], y_test[:50], scores, logregr, params, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now training using the best hyper-parameter selected above.  This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ini = datetime.datetime.now()\n",
    "print('Training...')\n",
    "clf = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(clf.score(X_test, y_test)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Types of Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "k=0\n",
    "i2w = id2word(test_sentences)\n",
    "error_counter = collections.Counter()\n",
    "for i in range(X_test.shape[0]):\n",
    "    correct_tag_id = y_test\n",
    "    if y_pred[i]!=y_test[i]:\n",
    "        k += 1\n",
    "        word = i2w[i]\n",
    "        error_counter[word] += 1\n",
    "print('Accuracy: {0:.4f}'.format((len(i2w)-k)/len(i2w)))\n",
    "print('Total errors/Total words: {}/{}\\n'.format(k,len(i2w)))\n",
    "print('Most common errors:',error_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates the \"training dynamics\" of the LR model: how fast does it improve as it keeps training. Originally, the difference between the test dataset and the cross-validation (on part of the test data) is large; as training proceeds, the gap reduces. This diagram is important to verify we do not have a case of over-fitting - where the model does \"very well\" on training data and does not improve on test data.  \n",
    "\n",
    "This computation takes a long time (as we keep training and evaluating multiple times to obtain the snapshots). It is not necessary to run the rest of the notebook so that you can safely skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_learning_curve(clf, 'Logistic Regression', X_train, y_train, ylim=(0.7, 1.01), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "name='Logistic Regression'\n",
    "plot_confusion_matrix(f1, cnf_matrix, target_names=classes, title='Confusion matrix for '+name+' classifier', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag an Unknown Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try our trained tagger on new sentences.  To tag a sentence given as a string, we must apply the following steps:\n",
    "* Tokenize the string into a list of tokens\n",
    "* Turn each token into a features dictionary (using the features used by our model)\n",
    "* Turn the list of feature dictionaries into vectors (using scikit-learn vectorization method)\n",
    "* Pass the resulting matrix (one row vector for each token) to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download the nltk model for sentence tokenizer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize('Word embeddings provide a dense representation of words and their relative meanings.')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = transform_test_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vec.transform([x[0] for x in X_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorized sentence is a sparse matrix with one row for each token and columns for the vectorized features\n",
    "# For example, if the vocabulary has 1000 unique words, the vectorized sentence will have 1000 columns for each word feature.\n",
    "# This is a very sparse matrix, where most of the values are zero.\n",
    "X_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.str_ is a subclass of str that is used to represent string arrays in NumPy.\n",
    "print('Here is what our LR tagger predicts for the test sentence:\\n',list(zip(tokens, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this process into a prediction function from a sentence encoded as a single string to a list of pairs (token, predicted_tag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "   tokens = nltk.word_tokenize(sentence)\n",
    "   X_features = transform_test_sentence(tokens)\n",
    "   X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "   pred = clf.predict(X_vectorized)\n",
    "   return list(zip(tokens, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence(\"Let me join the chorus of annoyance over Google's new toolbar , which, as noted in the linked article, commits just about every sin an online marketer could commit, and makes up a few new ones besides.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Hard Sentences\n",
    "\n",
    "Hard sentences are sentences that contain multiple wrongly predicted tags given our classifier.\n",
    "\n",
    "Write code to collect hard sentences given a classifier clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_in_sentence_prediction(clf, tagged_sentence):\n",
    "    \"\"\"Given a tagged sentence from the dataset, return the number of errors and the predicted tags.\"\"\"\n",
    "    errors = 0\n",
    "    tokens = [word for word, _ in tagged_sentence] \n",
    "    true_tags = [true_tag for _, true_tag in tagged_sentence] \n",
    "\n",
    "    X_features = transform_test_sentence(tokens)\n",
    "    X_vectorized = vec.transform([x[0] for x in X_features])\n",
    "    pred = clf.predict(X_vectorized)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if true_tags[i] != pred[i]:\n",
    "            errors += 1\n",
    "\n",
    "    return errors, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_sentences = []\n",
    "idx = 0\n",
    "for s in test_sentences:\n",
    "    errors, pred = errors_in_sentence_prediction(clf, s)\n",
    "    if errors > 0:\n",
    "        hard_sentences.append((s, errors, pred, idx))\n",
    "    idx += 1\n",
    "print(f'Number of sentences with errors: {len(hard_sentences)} out of {len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a histogram showing how the sentences in the test dataset are distributed in terms of prediction errors per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a histogram of the number of errors per sentence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([x[1] for x in hard_sentences], bins=16, edgecolor='black')\n",
    "plt.title('Number of errors per sentence')\n",
    "plt.xlabel('Number of errors')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction errors for sentences with more than 5 errors.\n",
    "for s in hard_sentences:\n",
    "    if s[1] > 5:\n",
    "        print(\" \".join(untag(s[0])))\n",
    "        print('Number of errors:', s[1])\n",
    "        for i in range(len(s[0])):\n",
    "            if s[0][i][1] != s[2][i]:\n",
    "                print(f'{s[0][i][0]:<20}  C: {s[0][i][1]:<12}  P: {s[2][i]:<12} **** Error')\n",
    "            else:\n",
    "                print(f'{s[0][i][0]:<23}  {s[0][i][1]:<12}')\n",
    "        print(40*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "1. Identify tokens that are misclassified more than 10 times in the test set. Print the sentences where the errors are predicted (about 100 sentences).\n",
    "2. Provide a possible reason why these errors are made by the tagger based on your understanding of the knowledge needed to correctly tag these tokens.\n",
    "3. Based on this error analysis, invent five sentences that are badly tagged. Explain what is your method to create these hard examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = collections.defaultdict(list)\n",
    "sen_ids = [] \n",
    "for i, s in enumerate(hard_sentences):\n",
    "    words = [word for word, _ in hard_sentences[i][0]]\n",
    "    full_s = ' '.join(words)\n",
    "    sen_ids.append(full_s)\n",
    "    for j in range(len(s[0])):\n",
    "        if s[0][j][1] != s[2][j]:\n",
    "            tokens[s[0][j][0]].append(i)\n",
    "\n",
    "sentences = set()\n",
    "print(\"Tokens with more than 10 errors:\")\n",
    "for token, lst in tokens.items():\n",
    "    if len(lst) > 10:\n",
    "        print(token)\n",
    "        for index in lst:\n",
    "            sentences.add(sen_ids[index])\n",
    "\n",
    "print(\"Total amount of sentences:\", len(sentences))\n",
    "\n",
    "print(\"Sentences where the errors are predicted:\\n\")\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# \n",
    "model = 'grok-3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Universal Dependencies POS Tagset (17 core tags) as an enum ---\n",
    "class UDPosTag(str, Enum):\n",
    "    ADJ = \"ADJ\"     # adjective\n",
    "    ADP = \"ADP\"     # adposition\n",
    "    ADV = \"ADV\"     # adverb\n",
    "    AUX = \"AUX\"     # auxiliary verb\n",
    "    CCONJ = \"CCONJ\" # coordinating conjunction\n",
    "    DET = \"DET\"     # determiner\n",
    "    INTJ = \"INTJ\"   # interjection\n",
    "    NOUN = \"NOUN\"   # noun\n",
    "    NUM = \"NUM\"     # numeral\n",
    "    PART = \"PART\"   # particle\n",
    "    PRON = \"PRON\"   # pronoun\n",
    "    PROPN = \"PROPN\" # proper noun\n",
    "    PUNCT = \"PUNCT\" # punctuation\n",
    "    SCONJ = \"SCONJ\" # subordinating conjunction\n",
    "    SYM = \"SYM\"     # symbol\n",
    "    VERB = \"VERB\"   # verb\n",
    "    X = \"X\"         # other / unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define more Pydantic models for structured output\n",
    "class TokenPOS(BaseModel):\n",
    "    text: str = Field(description=\"The token text\")\n",
    "    pos_tag: UDPosTag = Field(description=\"The Universal Dependencies POS tag\")\n",
    "\n",
    "class SentencePOS(BaseModel):\n",
    "    tokens: List[TokenPOS] = Field(description=\"List of tokens with their POS tags\")\n",
    "\n",
    "class TaggedSentences(BaseModel):\n",
    "    \"\"\"Represents a list of sentences with their tagged tokens.\"\"\"\n",
    "    sentences: List[SentencePOS] = Field(description=\"A list of sentences, each containing tagged tokens.\")\n",
    "\n",
    "# --- Configure the Grok API ---\n",
    "# Get a key https://console.x.ai/team \n",
    "# Use os.environ.get for production environments.\n",
    "# For Colab/AI Studio, you might use userdata.get\n",
    "# Example:\n",
    "# from google.colab import userdata\n",
    "# GROK_API_KEY = userdata.get('GROK_API_KEY')\n",
    "# genai.configure(api_key=GROK_API_KEY)\n",
    "\n",
    "# Make sure to replace \"YOUR_API_KEY\" with your actual key if running locally\n",
    "# and not using environment variables or userdata.\n",
    "try:\n",
    "    # Attempt to get API key from environment variable\n",
    "    def load_env_from_ini(filename):\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "    # Load the API key\n",
    "    load_env_from_ini(\"grok_key.ini\")\n",
    "    api_key = os.environ.get(\"GROK_API_KEY\")\n",
    "    if not api_key:\n",
    "        # Fallback or specific instruction for local setup\n",
    "        # Replace with your actual key if needed, but environment variables are safer\n",
    "        api_key = \"YOUR_API_KEY\"\n",
    "        if api_key == \"YOUR_API_KEY\":\n",
    "           print(\"⚠️ Warning: API key not found in environment variables. Using placeholder.\")\n",
    "           print(\"   Please set the GROK_API_KEY environment variable or replace 'YOUR_API_KEY' in the code.\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://api.x.ai/v1\",\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring API: {e}\")\n",
    "    print(\"Please ensure you have a valid API key set.\")\n",
    "    # Depending on the environment, you might want to exit here\n",
    "    # import sys\n",
    "    # sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentences_ud(text_to_tag: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on the input list of sentences using the Grok API and\n",
    "    returns the result structured according to the TaggedSentences Pydantic model.\n",
    "\n",
    "    Args:\n",
    "        text_to_tag: The sentence or text to be tagged.\n",
    "\n",
    "    Returns:\n",
    "        A SentencePOS object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions that outputs precise structured JSON.\n",
    "            Tag each token with Universal Dependencies (UD) POS tags: \n",
    "            ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner, INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun, PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other \n",
    "            Rules: \n",
    "               - Split on whitespace and punctuation (except in URLs, numbers, abbreviations)\n",
    "               - Split contractions: \"don't\" → [\"Do\", \"n't\"], \"it's\" → [\"It\", \"'s\"]\n",
    "               - Separate possessives: \"Elena's\" → [\"Elena\", \"'s\"]\n",
    "               - Split hyphenated compounds: \"search-engine\" → [\"search\", \"-\", \"engine\"]\n",
    "               - Keep punctuation as separate tokens\n",
    "               - Preserve numbers with internal periods/commas (e.g., 3.14, 1,000)\n",
    "               - Do not merge words except for contractions/clitics\n",
    "            Each sentence is wrapped with <sentence> and </sentence> tags.\n",
    "            Please process each sentence separately and include all sentences in your response.\n",
    "        \n",
    "            {text_to_tag}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text_to_tag},\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    # print(completion)\n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import concurrent.futures\n",
    "from typing import List, Optional\n",
    "\n",
    "# Define rate-limited API call function\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag(text_to_tag: str) -> Optional[TaggedSentences]:\n",
    "    return tag_sentences_ud(text_to_tag)\n",
    "\n",
    "def batch_tag_sentences_ud(sentences: List[str], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"Process sentences in parallel with rate limiting\"\"\"\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        marked_sentences = [f\"<sentence>{sentence}</sentence>\" for sentence in batch]\n",
    "        batch_text = \"\\n\".join(marked_sentences)\n",
    "        batches.append(batch_text)\n",
    "        \n",
    "    print(f\"Processing {len(batches)} batches with parallel workers...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag, batch): i \n",
    "                          for i, batch in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"✓ Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_sentences_ud(text_to_tag: str) -> Optional[TaggedSentences]:\n",
    "    \"\"\"\n",
    "    Performs POS tagging on pre-tokenized sentences represented as Python lists.\n",
    "    \n",
    "    Args:\n",
    "        text_to_tag: Pre-tokenized sentences formatted as \"Sentence X: [token1, token2, ...]\"\n",
    "        \n",
    "    Returns:\n",
    "        A TaggedSentences object containing the tagged tokens, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a specialized POS tagger following Universal Dependencies (UD_English-EWT) conventions.\n",
    "            \n",
    "            You will be given multiple pre-tokenized sentences in the format:\n",
    "            Sentence X: ['token1', 'token2', 'token3', ...]\n",
    "            \n",
    "            Your task is ONLY to assign the correct Universal Dependencies (UD) POS tag to each token.\n",
    "            Do NOT modify the tokenization in any way - use exactly the tokens provided in each list.\n",
    "            \n",
    "            Tags to use:\n",
    "            ADJ=adjective, ADP=adposition, ADV=adverb, AUX=auxiliary, CCONJ=coordinating conjunction, DET=determiner, \n",
    "            INTJ=interjection, NOUN=noun, NUM=numeral, PART=particle, PRON=pronoun, PROPN=proper noun, \n",
    "            PUNCT=punctuation, SCONJ=subordinating conjunction, SYM=symbol, VERB=verb, X=other\n",
    "            \n",
    "            Tagging guidelines:\n",
    "            - Auxiliary verbs (be, have, do, will, would, shall, should, can, could, may, might, must) should be tagged as AUX\n",
    "            - Main verbs should be tagged as VERB\n",
    "            - Proper nouns (names of people, places, organizations) should be tagged as PROPN\n",
    "            - Common nouns should be tagged as NOUN\n",
    "            - Articles (a, an, the) and demonstratives (this, that, these, those) should be tagged as DET\n",
    "            - Possessive markers ('s) should be tagged as PART\n",
    "            - Contractions like n't should be tagged as PART\n",
    "            \n",
    "\t\tPlease provide your output in the desired JSON format.\n",
    "            Input:\n",
    "            {text_to_tag}\n",
    "\t\t\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"grok-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text_to_tag},\n",
    "        ],\n",
    "        response_format=TaggedSentences,\n",
    "    )\n",
    "    res = completion.choices[0].message.parsed\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratelimit import limits, sleep_and_retry\n",
    "import concurrent.futures\n",
    "from typing import List, Optional\n",
    "\n",
    "# Define rate-limited API call function\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # 5 calls per second\n",
    "def rate_limited_tag(text_to_tag: str) -> Optional[TaggedSentences]:\n",
    "    return tag_sentences_ud(text_to_tag)\n",
    "\n",
    "def batch_tag_sentences_ud(tokenized_sentences: List[List[str]], batch_size: int = 15) -> List[TaggedSentences]:\n",
    "    \"\"\"Process pre-tokenized sentences in parallel with rate limiting\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of sentences, where each sentence is a list of tokens\n",
    "        batch_size: Number of sentences to include in each batch\n",
    "        \n",
    "    Returns:\n",
    "        List of TaggedSentences results\n",
    "    \"\"\"\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(tokenized_sentences), batch_size):\n",
    "        batch = tokenized_sentences[i:i+batch_size]\n",
    "        # Format each sentence as \"Sentence X: [token1, token2, ...]\"\n",
    "        formatted_sentences = []\n",
    "        for idx, tokens in enumerate(batch):\n",
    "            # Format tokens list with proper quotes around each token\n",
    "            tokens_str = \"['\" + \"', '\".join(tokens) + \"']\"\n",
    "            formatted_sentences.append(f\"Sentence {idx+1}: {tokens_str}\")\n",
    "        \n",
    "        batch_text = \"\\n\\n\".join(formatted_sentences)\n",
    "        batches.append(batch_text)\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches with parallel workers...\")\n",
    "    results = [None] * len(batches)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_batch = {executor.submit(rate_limited_tag, batch): i\n",
    "                          for i, batch in enumerate(batches)}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[batch_idx] = result\n",
    "                    print(f\"✓ Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed batch {batch_idx+1}/{len(batches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing batch {batch_idx+1}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # example_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    example_text = \"\"\"\n",
    "    What if Google expanded on its search-engine (and now e-mail) wares into a full-fledged operating system?\n",
    "    Google Search is a web search engine developed by Google LLC.\n",
    "    It does n't change the company 's intrinsic worth , and as the article notes , the company might be added to a major index once the shares get more liquid .\n",
    "    I 've been looking at the bose sound dock 10 i ve currently got a jvc mini hifi system , i was wondering what would be a good set of speakers .\n",
    "    which is the best burger chain in the chicago metro area like for example burger king portillo s white castle which one do you like the best ?\n",
    "    \"\"\"\n",
    "    # example_text = \"החתול המהיר קופץ מעל הכלב העצלן.\" # Example in Hebrew\n",
    "\n",
    "    print(f\"\\nTagging text: \\\"{example_text}\\\"\")\n",
    "\n",
    "    tagged_result = tag_sentences_ud(example_text)\n",
    "\n",
    "    if tagged_result:\n",
    "        print(\"\\n--- Tagging Results ---\")\n",
    "        for s in tagged_result.sentences:\n",
    "            # TODO: Retrieve tokens and tags from each sentence:\n",
    "            for token in s.tokens:\n",
    "                tag = token.pos_tag\n",
    "                token = token.text\n",
    "                # Handle potential None for pos_tag if model couldn't assign one\n",
    "                ctag = tag if tag is not None else \"UNKNOWN\"\n",
    "                print(f\"Token: {token:<15} {str(ctag)}\")\n",
    "                print(\"----------------------\")\n",
    "    else:\n",
    "        print(\"\\nFailed to get POS tagging results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hard_sentences = [s for s in hard_sentences if 1 <= s[1] <= 3]\n",
    "sentences = [\" \".join(untag(pairs)) for pairs, _, _ , _ in lr_hard_sentences]\n",
    "results_llm = batch_tag_sentences_ud(sentences, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pred = [[token.pos_tag for token in sentence.tokens] for result in results_llm for sentence in result.sentences]\n",
    "llm_tokens = [[token.text for token in sentence.tokens] for result in results_llm for sentence in result.sentences]\n",
    "\n",
    "print((word for word, _ in pairs) for pairs, _ in lr_hard_sentences[4])\n",
    "print([word for word in llm_tokens[4]])\n",
    "print(test_sentences[lr_hard_sentences[4][3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pred = [[token.pos_tag for token in sentence.tokens] for result in results_llm for sentence in result.sentences]\n",
    "llm_words = [[token.text for token in sentence.tokens] for result in results_llm for sentence in result.sentences]\n",
    "fixed_by_llm = 0\n",
    "new_errors_by_llm = 0\n",
    "llm_hard_sentences = []\n",
    "llm_error_data = []\n",
    "mismatches = 0\n",
    "\n",
    "for i, (s, errors, lr_tags, idx) in enumerate(lr_hard_sentences):\n",
    "    words = untag(s)\n",
    "    true_tags = untag_pos(s)\n",
    "    if i >= len(llm_pred):\n",
    "        print(f\"Skipping sentence {i} due to index out of range.\")\n",
    "        continue\n",
    "    pred_tags = llm_pred[i]\n",
    "    \n",
    "\n",
    "    # Sanity check: length of tokens should match\n",
    "    if len(true_tags) != len(pred_tags) or len(pred_tags) != len(words):\n",
    "        print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, pred={len(pred_tags)}\")\n",
    "        print(llm_words[i])\n",
    "        print(words)\n",
    "        mismatches += 1\n",
    "        continue\n",
    "\n",
    "    llm_errors = 0\n",
    "    for j in range(len(words)):\n",
    "        if true_tags[j] != pred_tags[j]:\n",
    "            llm_errors += 1\n",
    "            llm_error_data.append((words[j], true_tags[j], pred_tags[j]))\n",
    "        lr_wrong = lr_tags[j] != true_tags[j]\n",
    "        llm_wrong = pred_tags[j] != true_tags[j]\n",
    "        \n",
    "        if lr_wrong and not llm_wrong:\n",
    "            fixed_by_llm += 1\n",
    "        if not lr_wrong and llm_wrong:\n",
    "            new_errors_by_llm += 1\n",
    "            \n",
    "    if llm_errors > 0:\n",
    "        llm_hard_sentences.append((s, llm_errors))\n",
    "\n",
    "print(f'Number of sentences with errors (llm): {len(llm_hard_sentences)} out of {len(lr_hard_sentences)}')\n",
    "print(f'Number of sentences with errors (lr): {len(lr_hard_sentences)}')\n",
    "print(f\"✅ Errors fixed by LLM: {fixed_by_llm}\")\n",
    "print(f\"⚠️  New errors made by LLM: {new_errors_by_llm}\")\n",
    "print(f\"❗️ Mismatches in sentence length: {mismatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what we're working with and fix the extraction\n",
    "import collections\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Filter out None results\n",
    "results_llm = [r for r in results_llm if r is not None]\n",
    "\n",
    "# Inspect the first element to understand the structure\n",
    "if len(results_llm) > 0:\n",
    "    print(f\"Type of results_llm[0]: {type(results_llm[0])}\")\n",
    "    # If it has a 'sentences' attribute\n",
    "    if hasattr(results_llm[0], 'sentences'):\n",
    "        llm_pred = [[token.pos_tag for token in sentence.tokens] \n",
    "                    for result in results_llm \n",
    "                    for sentence in result.sentences]\n",
    "        llm_words = [[token.text for token in sentence.tokens] \n",
    "                     for result in results_llm \n",
    "                     for sentence in result.sentences]\n",
    "    # If it directly has a 'tokens' attribute\n",
    "    elif hasattr(results_llm[0], 'tokens'):\n",
    "        llm_pred = [[token.pos_tag for token in result.tokens] for result in results_llm]\n",
    "        llm_words = [[token.text for token in result.tokens] for result in results_llm]\n",
    "    else:\n",
    "        # If it's some other structure, print details to debug\n",
    "        print(\"Cannot determine structure. Attributes available:\", dir(results_llm[0]))\n",
    "        # Fallback option - skip further processing\n",
    "        print(\"Unable to extract predictions due to unexpected structure\")\n",
    "        llm_pred = []\n",
    "        llm_words = []\n",
    "else:\n",
    "    print(\"No valid results in results_llm\")\n",
    "    llm_pred = []\n",
    "    llm_words = []\n",
    "\n",
    "# Only proceed with metrics if we have predictions\n",
    "if len(llm_pred) > 0:\n",
    "    # Initialize counters and lists\n",
    "    fixed_by_llm = 0\n",
    "    new_errors_by_llm = 0\n",
    "    llm_hard_sentences = []\n",
    "    llm_error_data = []\n",
    "    mismatches = 0\n",
    "    \n",
    "    # For metrics calculation\n",
    "    all_true_tags = []\n",
    "    all_pred_tags = []\n",
    "    all_words = []\n",
    "    \n",
    "    for i, (s, errors, lr_tags, idx) in enumerate(lr_hard_sentences):\n",
    "        if i >= len(llm_pred):\n",
    "            print(f\"Skipping sentence {i} due to index out of range.\")\n",
    "            continue\n",
    "            \n",
    "        words = [word for word, _ in s]\n",
    "        true_tags = [true_tag for _, true_tag in s]\n",
    "        pred_tags = llm_pred[i]\n",
    "        \n",
    "        # Sanity check: length of tokens should match\n",
    "        if len(true_tags) != len(pred_tags) or len(pred_tags) != len(words):\n",
    "            print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, pred={len(pred_tags)}\")\n",
    "            print(llm_words[i] if i < len(llm_words) else \"Index out of range for llm_words\")\n",
    "            print(words)\n",
    "            mismatches += 1\n",
    "            continue\n",
    "        \n",
    "        # Add to the collections for metrics calculations\n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_pred_tags.extend(pred_tags)\n",
    "        all_words.extend(words)\n",
    "        \n",
    "        llm_errors = 0\n",
    "        for j in range(len(words)):\n",
    "            if true_tags[j] != pred_tags[j]:\n",
    "                llm_errors += 1\n",
    "                llm_error_data.append((words[j], true_tags[j], pred_tags[j]))\n",
    "                \n",
    "            lr_wrong = lr_tags[j] != true_tags[j]\n",
    "            llm_wrong = pred_tags[j] != true_tags[j]\n",
    "            \n",
    "            if lr_wrong and not llm_wrong:\n",
    "                fixed_by_llm += 1\n",
    "            if not lr_wrong and llm_wrong:\n",
    "                new_errors_by_llm += 1\n",
    "                \n",
    "        if llm_errors > 0:\n",
    "            llm_hard_sentences.append((s, llm_errors))\n",
    "    \n",
    "    # Print comparison metrics\n",
    "    print(f'Number of sentences with errors (llm): {len(llm_hard_sentences)} out of {len(lr_hard_sentences)}')\n",
    "    print(f'Number of sentences with errors (lr): {len(lr_hard_sentences)}')\n",
    "    print(f\"✅ Errors fixed by LLM: {fixed_by_llm}\")\n",
    "    print(f\"⚠️ New errors made by LLM: {new_errors_by_llm}\")\n",
    "    print(f\"❗️ Mismatches in sentence length: {mismatches}\")\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    if len(all_true_tags) > 0:\n",
    "        print(\"\\n--- LLM Tagger Token-Level Metrics ---\")\n",
    "        llm_accuracy = accuracy_score(all_true_tags, all_pred_tags)\n",
    "        llm_f1_macro = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "        print(f\"Accuracy: {llm_accuracy:.4f}\")\n",
    "        print(f\"F1-macro score: {llm_f1_macro:.4f}\")\n",
    "        \n",
    "        # Generate classification report\n",
    "        print(\"\\nClassification Report for LLM Tagger:\")\n",
    "        print(classification_report(all_true_tags, all_pred_tags, digits=4))\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        classes = sorted(list(set(all_true_tags)))\n",
    "        cnf_matrix = confusion_matrix(all_true_tags, all_pred_tags)\n",
    "        f1 = f1_score(all_true_tags, all_pred_tags, average='macro')\n",
    "        plot_confusion_matrix(f1, cnf_matrix, target_names=classes, \n",
    "                             title='Confusion matrix for LLM Tagger', normalize=False)\n",
    "        \n",
    "        # Count and display most common errors\n",
    "        error_counter = collections.Counter()\n",
    "        total_errors = 0\n",
    "        for i in range(len(all_true_tags)):\n",
    "            if all_true_tags[i] != all_pred_tags[i]:\n",
    "                total_errors += 1\n",
    "                word = all_words[i]\n",
    "                error_counter[word] += 1\n",
    "        \n",
    "        print(\"\\nFrequent Types of Mistakes:\")\n",
    "        print(f'Accuracy: {(len(all_true_tags)-total_errors)/len(all_true_tags):.4f}')\n",
    "        print(f'Total errors/Total words: {total_errors}/{len(all_true_tags)}\\n')\n",
    "        print('Most common errors:', error_counter.most_common(20))\n",
    "    else:\n",
    "        print(\"No data available to calculate metrics\")\n",
    "else:\n",
    "    print(\"No LLM predictions available to calculate metrics\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "\n",
    "# Plot the histogram for LR\n",
    "ax[0].hist([x[1] for x in lr_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[0].set_title(\"LR Tagger Error Histogram\")\n",
    "ax[0].set_xlabel(\"Number of Errors\")\n",
    "ax[0].set_ylabel(\"Number of Sentences\")\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Plot the histogram for LLM\n",
    "ax[1].hist([errors for s, errors in llm_hard_sentences], bins=range(1, 6), edgecolor='black', rwidth=0.8)\n",
    "ax[1].set_title(\"LLM Tagger Error Histogram\")\n",
    "ax[1].set_xlabel(\"Number of Errors\")\n",
    "ax[1].set_ylabel(\"Number of Sentences\")\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ErrorExplanation(BaseModel):\n",
    "    word: str\n",
    "    correct_tag: str\n",
    "    predicted_tag: str\n",
    "    explanation: str\n",
    "    category: str   \n",
    "\n",
    "def explain_tagging_errors(\n",
    "    errors: List[Tuple[str, str, \"UDPosTag\"]],\n",
    "    sentence_context: str = \"The error word appeared in a sentence. You may assume typical usage.\",\n",
    "    max_errors: int = 20,\n",
    "    delay: float = 1.0\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses Grok to explain POS tagging errors.\n",
    "\n",
    "    Args:\n",
    "        errors: A list of (word, predicted_tag, correct_tag) tuples.\n",
    "        sentence_context: Optional sentence to help Grok understand usage.\n",
    "        max_errors: Maximum number of errors to process.\n",
    "        delay: Seconds to wait between requests.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries with keys: word, correct_tag, predicted_tag, explanation, category.\n",
    "    \"\"\"\n",
    "    explanations = []\n",
    "\n",
    "    for word, predicted_tag, correct_tag in errors[:max_errors]:\n",
    "        prompt = f\"\"\"\n",
    "    You are a linguistics expert.\n",
    "\n",
    "    A POS tagging model made the following error:\n",
    "    - Word: {word}\n",
    "    - Correct tag: {correct_tag.value}\n",
    "    - Predicted tag: {predicted_tag}\n",
    "    - Sentence context: {sentence_context}\n",
    "\n",
    "    Explain in 1–3 sentences why this tagging error likely occurred, using a clear linguistic explanation (e.g., idiom, function word, capitalization confusion, named entity confusion, etc.).\n",
    "\n",
    "    Then provide a **concise category** that captures the main cause of the error.\n",
    "    ⚠️ The category must be **one of the following** (choose the best match):\n",
    "\n",
    "    - Function word misclassification\n",
    "    - Capitalization\n",
    "    - Named entity issue\n",
    "    - Punctuation influence\n",
    "    - Contextual ambiguity\n",
    "    - Preposition/Adverb confusion\n",
    "    - Model bias\n",
    "    - Idiomatic expression\n",
    "    - Word frequency bias\n",
    "    - Tokenization mismatch\n",
    "\n",
    "    Do **not** create new category names. Only pick from the list above.\n",
    "\n",
    "    Return a JSON object with:\n",
    "    - word\n",
    "    - correct_tag\n",
    "    - predicted_tag\n",
    "    - explanation\n",
    "    - category\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format=ErrorExplanation,\n",
    "            )\n",
    "            parsed: ErrorExplanation = completion.choices[0].message.parsed\n",
    "            explanations.append(parsed.dict())  # Convert to plain dict\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining word '{word}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def format_explanations_as_json_block(explanations: list) -> str:\n",
    "    formatted = []\n",
    "\n",
    "    for e in explanations:\n",
    "        word = e[\"word\"].upper()\n",
    "        correct = e[\"correct_tag\"]\n",
    "        predicted = e[\"predicted_tag\"]\n",
    "\n",
    "        # Try to extract short category + tag pair\n",
    "        if \"/\" in e[\"category\"]:\n",
    "            parts = e[\"category\"].split(\"/\")\n",
    "            if len(parts) == 2:\n",
    "                tag1, tag2 = parts[0].strip(), parts[1].split()[0].strip()  # split off 'ambiguity'\n",
    "                category = f\"Ambiguity ({tag1}/{tag2})\"\n",
    "            elif len(parts) == 3:  # e.g. \"ADP/ADV ambiguity\"\n",
    "                tag1, tag2 = parts[0].strip(), parts[1].strip()\n",
    "                category = f\"Ambiguity ({tag1}/{tag2})\"\n",
    "            else:\n",
    "                category = e[\"category\"].capitalize()\n",
    "        else:\n",
    "            category = e[\"category\"].capitalize()\n",
    "\n",
    "        formatted.append({\n",
    "            \"word\": word,\n",
    "            \"correct_tag\": correct,\n",
    "            \"predicted_tag\": predicted,\n",
    "            \"explanation\": e[\"explanation\"].strip(),\n",
    "            \"category\": category\n",
    "        })\n",
    "\n",
    "    return \"JSON\\n\" + json.dumps(formatted, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = explain_tagging_errors(llm_error_data[0:20])\n",
    "print(format_explanations_as_json_block(explanations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "category_counter = Counter([e[\"category\"] for e in explanations])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort categories by frequency\n",
    "sorted_items = sorted(category_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "categories, counts = zip(*sorted_items)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(categories, counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Number of Errors\")\n",
    "plt.title(\"Error Categories in LLM Tagging\")\n",
    "plt.gca().invert_yaxis()  # Largest on top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for cat, count in category_counter.most_common():\n",
    "    print(f\"{cat}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import List\n",
    "from pydantic import BaseModel, RootModel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Define schemas\n",
    "class SyntheticSentence(BaseModel):\n",
    "    sentence: List[str]\n",
    "    tags: List[str]\n",
    "    categories: List[str]  # ✅ added\n",
    "\n",
    "class SyntheticBatch(RootModel[List[SyntheticSentence]]):\n",
    "    pass\n",
    "\n",
    "# ✅ Predefined error categories\n",
    "error_categories = [\n",
    "    \"Function word misclassification\",\n",
    "    \"Capitalization\",\n",
    "    \"Named entity issue\",\n",
    "    \"Punctuation influence\",\n",
    "    \"Contextual ambiguity\",\n",
    "    \"Preposition/Adverb confusion\",\n",
    "    \"Model bias\",\n",
    "    \"Idiomatic expression\",\n",
    "    \"Word frequency bias\",\n",
    "    \"Tokenization mismatch\"\n",
    "]\n",
    "\n",
    "# ✅ Build prompt\n",
    "def build_synthetic_prompt(categories: List[str]) -> str:\n",
    "    joined = \", \".join(categories)\n",
    "    return f\"\"\"\n",
    "Generate 2 English sentences that demonstrate POS tagging challenges involving: {joined}.\n",
    "\n",
    "For each sentence, return:\n",
    "- A list of tokens\n",
    "- Their corresponding UD POS tags\n",
    "\n",
    "Return a valid JSON list like:\n",
    "[\n",
    "  {{\n",
    "    \"sentence\": [...],\n",
    "    \"tags\": [...]\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Only return JSON. Do not explain.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Request a single batch with retry and category attachment\n",
    "def generate_batch(categories: List[str], max_retries=3) -> List[SyntheticSentence]:\n",
    "    prompt = build_synthetic_prompt(categories)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=\"grok-3\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                response_format=SyntheticBatch,\n",
    "            )\n",
    "            results = completion.choices[0].message.parsed.root\n",
    "            # ✅ attach categories to each result\n",
    "            for r in results:\n",
    "                r.categories = categories\n",
    "            return [r for r in results if len(r.sentence) == len(r.tags)]\n",
    "        except Exception as e:\n",
    "            wait = 2 + random.uniform(0, 2)\n",
    "            print(f\"[Retry {attempt+1}] Error: {e} — waiting {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    print(\"❌ Failed after retries.\")\n",
    "    return []\n",
    "\n",
    "# ✅ Run in parallel\n",
    "def run_parallel_generation(n_batches: int = 200, n_threads: int = 5) -> List[SyntheticSentence]:\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(generate_batch, random.sample(error_categories, k=3))\n",
    "            for _ in range(n_batches)\n",
    "        ]\n",
    "        for future in tqdm(as_completed(futures), total=n_batches):\n",
    "            batch = future.result()\n",
    "            all_results.extend(batch)\n",
    "    return all_results\n",
    "\n",
    "# ✅ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    n_batches = 100   # 100 batches × 2 sentences = ~200\n",
    "    n_threads = 5     # Adjust as needed\n",
    "\n",
    "    results = run_parallel_generation(n_batches=n_batches, n_threads=n_threads)\n",
    "\n",
    "    print(f\"\\n✅ Done! Generated {len(results)} synthetic sentences.\")\n",
    "\n",
    "    # ✅ Print some examples with categories\n",
    "    for i, r in enumerate(results[:10]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(\"Sentence: \", \" \".join(r.sentence))\n",
    "        print(\"Tags:     \", r.tags)\n",
    "        print(\"Categories:\", r.categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_synthetic_to_tagged_sentences(synthetic: List[SyntheticSentence]):\n",
    "    return [list(zip(s.sentence, s.tags)) for s in synthetic]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    n_batches = 100\n",
    "    n_threads = 5\n",
    "    # print(\"Generating synthetic data...\")\n",
    "    # results = run_parallel_generation(n_batches=n_batches, n_threads=n_threads)\n",
    "\n",
    "    # print(f\"\\n Done! Generated {len(results)} synthetic sentences.\")\n",
    "    # for i, r in enumerate(results[:5]):\n",
    "    #     print(f\"\\n--- Example {i+1} ---\")\n",
    "    #     print(\"Sentence: \", \" \".join(r.sentence))\n",
    "    #     print(\"Tags:     \", r.tags)\n",
    "    #     print(\"Categories:\", r.categories)\n",
    "\n",
    "    # Convert to training format and combine\n",
    "    synthetic_tagged = convert_synthetic_to_tagged_sentences(results)\n",
    "    train_augmented = train_sentences + synthetic_tagged * 3 \n",
    "    random.shuffle(train_augmented)\n",
    "\n",
    "    # Vectorize with your existing function\n",
    "    print(\"Vectorizing data...\")\n",
    "    X_train_synth, y_train_synth, X_val_synth, y_val_synth, X_test_synth, y_test_synth, vectorizer_synth = vectorize(train_augmented, val_sentences, test_sentences)\n",
    "\n",
    "    # Train the LR tagger\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    t_ini = datetime.datetime.now()\n",
    "    clf_synth = LogisticRegression(C=20, solver='liblinear', random_state=13)\n",
    "    clf_synth.fit(X_train_synth, y_train_synth)\n",
    "    t_fin = datetime.datetime.now()\n",
    "    print(f\"Training completed in {(t_fin - t_ini).total_seconds():.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_orig = clf.predict(X_test)\n",
    "y_pred_synth = clf_synth.predict(X_test_synth)\n",
    "\n",
    "# Track positions where the original was wrong but the synthetic fixed it, and vice versa\n",
    "fixed_by_synth = []\n",
    "regressed_by_synth = []\n",
    "unchanged_errors = []\n",
    "\n",
    "for i, (true, pred_orig, pred_synth) in enumerate(zip(y_test_synth, y_pred_orig, y_pred_synth)):\n",
    "    if pred_orig != true and pred_synth == true:\n",
    "        fixed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig == true and pred_synth != true:\n",
    "        regressed_by_synth.append((i, true, pred_orig, pred_synth))\n",
    "    elif pred_orig != true and pred_synth != true:\n",
    "        unchanged_errors.append((i, true, pred_orig, pred_synth))\n",
    "\n",
    "print(\"Contrastive Error Analysis\")\n",
    "print(f\"Fixed errors by synthetic model: {len(fixed_by_synth)}\")\n",
    "print(f\"New errors introduced by synthetic model: {len(regressed_by_synth)}\")\n",
    "print(f\"Unchanged errors (both models wrong): {len(unchanged_errors)}\")\n",
    "\n",
    "print(\"\\n--- Original Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_orig, digits=3))\n",
    "\n",
    "print(\"\\n--- Synthetic-Augmented Model ---\")\n",
    "print(classification_report(y_test_synth, y_pred_synth, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_failed_with_original = []\n",
    "total_errors = 0\n",
    "\n",
    "for i, sentence in enumerate(test_sentences[:10]):\n",
    "    true_tags = untag_pos(sentence)\n",
    "    errors_org = 0\n",
    "    errors_tok = 0  \n",
    "\n",
    "    tokenized_res = tag_sentences_ud(\" \".join(untag(sentence)))\n",
    "    original_res = tag_sentences_ud(test_original[i])\n",
    "\n",
    "    tokenized_tags = [[token.pos_tag.value for token in sentence.tokens] for sentence in tokenized_res.sentences]\n",
    "    tokenized_words = [[token.text for token in sentence.tokens] for sentence in tokenized_res.sentences]\n",
    "    original_tags = [[token.pos_tag.value for token in sentence.tokens] for sentence in original_res.sentences]\n",
    "    original_words = [[token.text for token in sentence.tokens] for sentence in original_res.sentences]\n",
    "    print(tokenized_words)\n",
    "    print(original_words)\n",
    "    print(untag(sentence))\n",
    "    \n",
    "    if len(true_tags) != len(original_tags[0]) or len(true_tags) != len(tokenized_tags[0]) or len(tokenized_tags[0]) != len(original_tags[0]):\n",
    "        print(f\"Length mismatch in sentence {i}: true={len(true_tags)}, original={len(original_tags[0])}, tokenized={len(tokenized_tags[0])}\")\n",
    "        break\n",
    "    \n",
    "    for j in range(len(true_tags)):\n",
    "        if true_tags[j] != original_tags[0][j]:\n",
    "            errors_org += 1\n",
    "        if true_tags[j] != tokenized_tags[0][j]:\n",
    "            errors_tok += 1\n",
    "\n",
    "    if errors_org > 0 and errors_tok == 0:\n",
    "        sentences_failed_with_original.append(sentence)  \n",
    "    else:\n",
    "        print(f\"original: {errors_org} tokenized: {errors_tok}\")  \n",
    "    if errors_org > 0:\n",
    "        total_errors += 1\n",
    "    \n",
    "\n",
    "print(f\"Sentences that failed with the original text but not with the tokenized version: {len(sentences_failed_with_original)}\")\n",
    "print(f\"Performance on original text: {total_errors} out of {len(test_sentences[:10])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
